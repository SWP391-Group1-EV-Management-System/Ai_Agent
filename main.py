
"""
üöÄ Production-Ready LangGraph AI Worker with RabbitMQ & Redis
Enterprise-grade distributed agent system for LangGraph 1.0.0

Author: Your Team
Version: 2.0.0-production (LangGraph 1.0.0)
"""

import asyncio
import json
import selectors
import uuid
import os
import logging
import sys
import signal
from typing import Dict, Any, List, Optional, Annotated
from datetime import datetime
from hashlib import md5
from contextlib import suppress, asynccontextmanager

# Message Broker & Cache
import aio_pika
from aio_pika import DeliveryMode, ExchangeType
import redis.asyncio as aioredis

# Web Framework
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Environment & Config
from dotenv import load_dotenv

# Database
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

# LangGraph & LangChain - UPDATED FOR 1.0.0
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition

# Conditional import for checkpointer
try:
    from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("‚ö†Ô∏è  PostgreSQL checkpointer not available. Running without persistence.")
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from typing_extensions import TypedDict

# Tools
from tools.register_tools import TOOLS

# ==================== CONFIGURATION ====================
load_dotenv()
#l·∫•y c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
class Config:
    """Centralized configuration with validation"""
    # Infrastructure
    RABBITMQ_URL: str = os.getenv("RABBITMQ_URL")
    REDIS_URL: str = os.getenv("REDIS_URL")
    DATABASE_URL: str = os.getenv("DATABASE_URL")
    CHECKPOINTER_DB_DSN: str = os.getenv("CHECKPOINTER_DB_DSN") 
    # Sharding
    SHARD_COUNT: int = int(os.getenv("SHARD_COUNT", "4"))
    AI_QUEUE_PREFIX: str = os.getenv("AI_QUEUE_PREFIX", "ai_jobs_")
    DLX_NAME: str = os.getenv("DLX", "ai_dlx")
    DLQ_NAME: str = "ai_jobs.dlq"
    
    # Performance
    LLM_CONCURRENCY: int = int(os.getenv("LLM_CONCURRENCY", "5"))
    PREFETCH_COUNT: int = int(os.getenv("PREFETCH_COUNT", "10"))
    DB_POOL_SIZE: int = int(os.getenv("DB_POOL_SIZE", "20"))
    DB_MAX_OVERFLOW: int = int(os.getenv("DB_MAX_OVERFLOW", "10"))
    
    # Retry & Timeout
    MAX_RETRIES: int = int(os.getenv("MAX_RETRIES", "5"))
    LOCK_TTL_MS: int = int(os.getenv("LOCK_TTL_MS", "30000"))
    JOB_TIMEOUT_SEC: int = int(os.getenv("JOB_TIMEOUT_SEC", "300"))
    
    # LLM
    USE_DUMMY: bool = os.getenv("USE_DUMMY", "false").lower() == "true"
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY")
    LLM_MODEL: str = os.getenv("LLM_MODEL", "gemini-2.0-flash-exp")
    LLM_TEMPERATURE: float = float(os.getenv("LLM_TEMPERATURE", "0.3"))
    LLM_MAX_TOKENS: int = int(os.getenv("LLM_MAX_TOKENS", "2048"))
    
    # Memory & History
    MAX_HISTORY_MESSAGES: int = int(os.getenv("MAX_HISTORY_MESSAGES", "20"))
    CHECKPOINT_TTL_SEC: int = int(os.getenv("CHECKPOINT_TTL_SEC", "3600"))
    
    # System Prompt t·ª´ file txt
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT: str = f.read()
    
    @classmethod
    def validate(cls):
        """Validate critical configuration"""
        required = ["RABBITMQ_URL", "REDIS_URL", "DATABASE_URL"]
        missing = [k for k in required if not getattr(cls, k)]
        # n·∫øu c√°c bi·∫øn m√¥i tr∆∞·ªùng quan tr·ªçng b·ªã thi·∫øu, n√©m l·ªói
        if missing:
            raise ValueError(f"Missing required config: {', '.join(missing)}")
        
        if not cls.USE_DUMMY and not cls.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY required when USE_DUMMY=false")

config = Config() #t·∫°o instance config
config.validate() #ki·ªÉm tra c·∫•u h√¨nh quan tr·ªçng tr∆∞·ªõc khi ch·∫°y

# ==================== LOGGING ====================
# c·∫•u h√¨nh cho h·ªá th·ªëng ghi log
logging.basicConfig( 
    level=logging.INFO, #ch·ªâ l·∫•y log t·ª´ m·ª©c INFO tr·ªü l√™n b·ªè qua DEBUG (WARNING, ERROR, CRITICAL)
    # gi·ªëng java LoggerFactory 
    format="%(asctime)s [%(levelname)s] [%(name)s:%(lineno)d] %(message)s",
    #asctime l√† th·ªùi gian | levelname l√† m·ª©c ƒë·ªô log (INFO, ...) | name l√† t√™n logger | lineno l√† s·ªë d√≤ng trong code | message l√† n·ªôi dung log
    datefmt="%Y-%m-%d %H:%M:%S", #ƒë·ªãnh d·∫°ng l·∫°i th·ªùi gian
    # 2025-10-23 17:33:05 v√≠ d·ª• 
    handlers=[
        logging.FileHandler("ai_worker.log", encoding="utf-8"), #ghi v√†o file
        logging.StreamHandler(sys.stdout) #ghi ra console
    ]
)
logger = logging.getLogger(__name__) #kh·ªüi t·∫°o logger trong file hi·ªán t·∫°i 

# ==================== DATABASE ====================
engine = create_async_engine( # s·ª≠ d·ª•ng k·∫øt n·ªëi b·∫•t ƒë·ªìng b·ªô ( kh√¥ng g√¢y ch·∫∑n ch∆∞∆°ng tr√¨nh khi ƒëang truy xu·∫•t)
    config.DATABASE_URL, # link k·∫øt n·ªëi db
    pool_size=config.DB_POOL_SIZE, #s·ªë k·∫øt n·ªëi t·ªëi ƒëa trong pool 
    # (gi·ªëng nh∆∞ c·ªïng ƒëi·ªán tho·∫°i c√¥ng c·ªông, ng∆∞·ªùi n√†y s·ª≠ d·ª•ng tr√£ l·∫°i ƒë·∫øn ng∆∞·ªùi kh√°c s·ª≠ d·ª•ng, c√≥ 20 c√°i ƒët)
    max_overflow=config.DB_MAX_OVERFLOW, #ph√≤ng tr∆∞·ªùng h·ª£p qu√° t·∫£i
    # (n·∫øu 20 c√°i ƒët ƒë·ªÅu b·∫≠n, c√≥ th·ªÉ t·∫°m th·ªùi m∆∞·ª£n th√™m 10 c√°i n·ªØa ƒë·ªÉ ph·ª•c v·ª• kh√°ch h√†ng)
    pool_pre_ping=True, #ki·ªÉm tra k·∫øt n·ªëi c√≤n s·ªëng tr∆∞·ªõc khi s·ª≠ d·ª•ng
    echo=False #kh√¥ng in c√¢u l·ªánh SQL ra log ( th·ª´a), ƒë·ªïi true n·∫øu mu·ªën xem chi ti·∫øt ƒë·ªÉ debug
)
AsyncSessionLocal = async_sessionmaker( #EntityManagerFactory trong java
    # t·∫°o session giao ti·∫øp b·∫•t ƒë·ªìng b·ªô v·ªõi db
    engine, # c·∫•u h√¨nh ·ªü tr√™n 
    class_=AsyncSession, #b·∫•t ƒë·ªìng b·ªô
    expire_on_commit=False # tr√°nh x√≥a d·ªØ li·ªáu trong session, ƒë·ªÉ truy xu·∫•t l·∫°i nhanh h∆°n
)

# ==================== LLM ====================
class DummyLLM: 
    """Dummy LLM d√πng ƒë·ªÉ test ƒë·ªÉ kh√¥ng t·ªën token"""
    async def ainvoke(self, messages):
        await asyncio.sleep(1) # tƒÉng ƒë·ªô tr·ªÖ
        last_msg = messages[-1] #l·∫•y tin nh·∫Øn cu·ªëi c√πng v√¨ trong ƒë·ªëi t∆∞·ª£ng messages s·∫Ω c√≥ nhi·ªÅu fiedls kh√°c nhau, fiedl cu·ªëi c√πng l√† humanMessage 
        content = last_msg.content if hasattr(last_msg, 'content') else str(last_msg) # in n·ªôi dung tin nh·∫Øn t·ª©c ph·∫ßn content, n·∫øu kh√¥ng th√¨ in h√™t ra tr√°nh l·ªói
        return AIMessage(content=f"[DUMMY] Response to: {content[:100]}...") # tr·∫£ v·ªÅ tin nh·∫Øn AIMessage v·ªõi n·ªôi dung gi·∫£ l·∫≠p
    
    def bind_tools(self, tools): # kh√¥ng g·∫Øn tools cho dummy 
        return self

# X√°c th·ª±c g·ªçi dummy ho·∫∑c Google Gemini
if config.USE_DUMMY:
    llm = DummyLLM()
    logger.info("üß™ Using DUMMY LLM (no API calls)")
else:
    llm = ChatGoogleGenerativeAI(
        model=config.LLM_MODEL, #g·ªçi model gemini c·ª• th·ªÉ 
        temperature=config.LLM_TEMPERATURE, # ƒë·ªô s√°ng t·∫°o c·ªßa c√¢u tr√£ l·ªùi (ƒë·ªÉ ·ªü m·ª©c an to√†n tr√°nh b·ªãa chuy·ªán)
        google_api_key=config.GOOGLE_API_KEY, #API key
        max_tokens=config.LLM_MAX_TOKENS, # gi·ªõi h·∫°n s·ªë token trong c√¢u tr·∫£ l·ªùi
        top_p=0.95, #ƒë·ªô r·ªông c·ªßa ph√¢n ph·ªëi x√°c su·∫•t (gi√∫p ƒëa d·∫°ng c√¢u tr·∫£ l·ªùi)
    )
    logger.info(f"ü§ñ Using Google Gemini: {config.LLM_MODEL}")

# g·∫Øn tools cho LLM gemini
llm_with_tools = llm.bind_tools(TOOLS) if not config.USE_DUMMY else llm
#TOOLS ƒë√£ ƒë∆∞·ª£c g·∫Øn s·∫µn v√† import 
# Semaphore for LLM concurrency control
llm_sem = asyncio.Semaphore(config.LLM_CONCURRENCY) # cho ph√©p s·ªë l∆∞·ª£ng LLM ƒë·ªìng th·ªùi m·ªôt l√∫c ch·∫°y l√† LLM_CONCURRENCY

# ==================== AGENT STATE (LANGGRAPH 1.0.0) ====================
from operator import add

class AgentState(TypedDict):
    """Agent state for LangGraph 1.0.0 with message reducer"""
    messages: Annotated[List[BaseMessage], add]  # ‚úÖ Use 'add' operator to APPEND messages
    user_id: str
    thread_id: str

# ==================== AGENT NODES ====================
async def call_model(state: AgentState): #state ·ªü ƒë√¢y l√† dict gi·ªëng nh∆∞ Map trong java
    """Tr√°i tim c·ªßa agent - g·ªçi LLM v·ªõi messages ƒë√£ c√≥"""
    messages = state["messages"] #l·∫•y value c·ªßa key messages trong state, t·ª©c l√† c√°c tin nh·∫Øn m·ªõi ƒë∆∞·ª£c g·ª≠i ƒë·∫øn agent
    
    # g√°n system prompt v√†o systemMessages (SystemMessage, HumanMessage, AIMessage)
    system_msg = SystemMessage(content=config.SYSTEM_PROMPT)
    
    # Remove any existing system messages to avoid duplicates
    messages_without_system = [m for m in messages if not isinstance(m, SystemMessage)] 
    # x√≥a systemMessage c≈© n·∫øu c√≥, m s·∫Ω l∆∞u nh·ªØng ph·∫ßn c√≤n l·∫°i m√† kh√¥ng ph·∫£i systemMessage
    
    # ‚úÖ Filter out empty messages that could cause Gemini errors
    valid_messages = [] # danh s√°ch l∆∞u tr·ªØ c√°c message h·ª£p l·ªá ( l·ªçc c√°c message r·ªóng, tr√°nh l·ªói cho gemini)
    for m in messages_without_system:
        if hasattr(m, 'content') and m.content:
            valid_messages.append(m) #(hasattr(m, 'content') h√†m ki·ªÉm tra m c√≥ thu·ªôc t√≠nh content kh√¥ng, v√† m.content ki·ªÉm tra content c√≥ r·ªóng kh√¥ng)
        elif hasattr(m, 'tool_calls') and m.tool_calls:
            valid_messages.append(m)
        else:
            logger.warning(f"‚ö†Ô∏è Skipping empty message: {type(m).__name__}")
    
    # Add system prompt at the start
    messages_with_system = [system_msg] + valid_messages
    
    # b·∫Øt l·ªói n·∫øu ch·ªâ c√≥ m·ªói systemMessage m√† kh√¥ng c√≥ message n√†o kh√°c 
    if len(messages_with_system) < 2:
        logger.error(f"‚ùå Not enough messages to send to LLM: {len(messages_with_system)}") # s·ªë ph·∫ßn t·ª≠ trong messages_with_system
        return {"messages": [AIMessage(content="Xin l·ªói anh/ch·ªã, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu ·∫°.")]}
    
    # Log
    logger.debug(f"üîç Sending {len(messages_with_system)} messages to LLM")
    
    # Invoke LLM with tools
    try:
        async with llm_sem: #c√≤n ch·ªó tr·ªëng trong semaphore th√¨ m·ªõi g·ªçi LLM
            response = await llm_with_tools.ainvoke(messages_with_system) # g·ª≠i t·∫•t c·∫£ message ƒë√£ c√≥ (bao g·ªìm systemMessage v√† userMessage) ƒë·∫øn LLM ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        return {"messages": [response]} 
    except Exception as e:
        logger.error(f"‚ùå LLM invocation error: {e}")
        return {"messages": [AIMessage(content="Xin l·ªói anh/ch·ªã, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu ·∫°.")]}

def should_continue(state: AgentState):
    """Ki·ªÉm tra xem agent c√≥ c·∫ßn g·ªçi tools hay d·ª´ng l·∫°i"""
    messages = state["messages"] # l·∫•y tin nh·∫Øn 
    last_message = messages[-1] # l·∫•y tin nh·∫Øn cu·ªëi c√πng m√† AI v·ª´a t·∫°o ra 
    
    # ki·ªÉm tra xem n√≥ c√≥ mu·ªën g·ªçi tools hay kh√¥ng, kh√¥ng th√¨ cho n√≥ end 
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    # end state ·ªü ƒë√¢y
    return END

# ==================== LANGGRAPH AGENT (1.0.0) ====================
async def create_agent_executor():
    """
    Create LangGraph agent with PostgreSQL checkpointer
    Updated for LangGraph 1.0.0
    """
    # PostgreSQL checkpointer
    checkpointer = None
    if POSTGRES_AVAILABLE and config.DATABASE_URL: #kh·ªüi t·∫°o checkpointer tƒÉng t√≠nh b·ªÅn v·ªØng
        try:
            checkpointer_cm = AsyncPostgresSaver.from_conn_string(config.DATABASE_URL) # t·∫°o context manager cho checkpointer
            checkpointer = await checkpointer_cm.__aenter__() # kh·ªüi t·∫°o checkpointer b·∫•t ƒë·ªìng b·ªô ( l√∫c n√†y context manager ƒë√£ v√†o tr·∫°ng th√°i active) # __aenter__() l√† h√†m ƒë·ªÉ chu·∫©n b·ªã t√†i nguy√™n, kh·ªüi ƒë·ªông ƒë·ªÉ s·ª≠ d·ª•ng 
            await checkpointer.setup() # s·∫µn s√†ng s·ª≠ d·ª•ng 
            logger.info("‚úÖ PostgreSQL checkpointer enabled")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to setup PostgreSQL checkpointer: {e}")
            logger.info("üìù Running without persistence")
            checkpointer = None
    else:
        logger.info("üìù Running without checkpointer (no persistence)")
    
    # T·∫°o tool node 
    tool_node = ToolNode(TOOLS) 
    
    # ‚úÖ CRITICAL FIX: Wrap tool node to maintain conversation flow
    async def tool_node_func(state: AgentState):
        print("\n" + "=" * 80)
        print("üîß TOOL NODE CALLED")
        original_messages = state['messages'] # l·∫•y to√†n b·ªô messages hi·ªán t·∫°i tr∆∞·ªõc khi g·ªçi tool
        print(f"üìä Messages before tools: {len(original_messages)}")
        
        # Debug: li·ªát k√™ tin nh·∫Øn ƒëang c√≥ tr∆∞·ªõc khi g·ªçi tool
        for i, msg in enumerate(original_messages):
            msg_type = type(msg).__name__
            print(f"  [{i}] {msg_type}")
            ''' [0] SystemMessage
                [1] HumanMessage
                [2] AIMessage'''
        
        try:
            # g·ªçi tools ( ·ªü b∆∞·ªõc n√†y LLM ƒë√£ ph√¢n t√≠ch v√† ƒë∆∞a ra tools c·∫ßn v√† tham s·ªë ph·ª•c v·ª• cho tools r·ªìi)
            result = await tool_node.ainvoke(state) # ainvoke n√†y l√† h√†m c·ªßa thu vi·ªán langgraph
            tool_messages = result.get('messages', [])
            
            print(f"‚úÖ Tool execution completed")
            print(f"üì¶ Tool returned {len(tool_messages)} new messages")
            
            # Debug tool results
            for msg in tool_messages:
                msg_type = type(msg).__name__ # in ra ki·ªÉu message "__name__" tr√£ v·ªÅ t√™n l·ªõp d·∫°ng string
                content_preview = str(msg.content)[:100] if hasattr(msg, 'content') else str(msg)[:100] # in ra n·ªôi dung c·ªßa toolMessage n·∫øu c√≥ ho·∫∑c c·∫£ msg, content l√† ph·∫ßn tin nh·∫Øn ch√≠nh
                print(f"   ‚Üí {msg_type}: {content_preview}...")
            
            
            print(f"üîó Returning {len(tool_messages)} tool messages (LangGraph will merge)")
            print("=" * 80 + "\n")
            
            # # m√¥ h√¨nh ch·∫°y b·∫•t ƒë·ªìng b·ªô n√†y s·∫Ω l√†m nhi·ªÅu node (worker) ch·∫°y song song ƒë·ªÉ x·ª© l√Ω v·∫•n ƒë·ªÅ
            # do ƒë√≥ node n√†y c·∫ßn tr√£ ƒë√∫ng k·∫øt qu·∫£ v·ªÅ cho node ch√≠nh (agent) ƒë·ªÉ n√≥ t·ªïng h·ª£p, ƒë·ªÉ tr√°nh b·ªã ghi ƒë√® ch·ªâ g·ª≠i ph·∫ßn content toolMessage
            return {"messages": tool_messages}
            
        except Exception as e:
            print(f"‚ùå TOOL NODE ERROR: {e}")
            logger.error(f"Tool node error: {e}", exc_info=True)
            raise
    
    # Create workflow with Annotated reducer for messages
    from typing import Annotated
    from operator import add
    
    # ‚úÖ CRITICAL FIX: Define state with proper reducer
    class FixedAgentState(TypedDict):
        """ Agent state v·ªõi reducer ƒë·ªÉ n·ªëi messages ƒë√∫ng c√°ch """
        messages: Annotated[List[BaseMessage], add]  # Use 'add' to APPEND messages
        user_id: str
        thread_id: str
    
    workflow = StateGraph(FixedAgentState) # khai b√°o workflow v·ªõi state ƒë√£ fix
    #StateGraph ƒë·∫£m b·∫£o lu·ªìng c√¥ng vi·ªác c·ªßa agent ƒë∆∞·ª£c qu·∫£n l√Ω ƒë√∫ng c√°ch
    # Add nodes
    workflow.add_node("agent", call_model) #node n√†y g·ªçi LLM tr√£ v·ªÅ c√¢u tr·∫£ l·ªùi v√† toolCalls n·∫øu c√≥
    workflow.add_node("tools", tool_node_func) #node n√†y g·ªçi tools n·∫øu LLM y√™u c·∫ßu
    
    # b·∫Øt ƒë·∫ßu lu·ªìng c√¥ng vi·ªác
    workflow.add_edge(START, "agent") 
    # ki·ªÉm tra c√≥ toolMessage kh√¥ng ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·∫°y ti·∫øp hay d·ª´ng
    workflow.add_conditional_edges(
        "agent",
        should_continue, # check c√≥ toolMessage kh√¥ng
        {
            "tools": "tools",
            END: END # tr√£ end th√¨ d·ª´ng v√¨ l√∫c n√†y agent ƒë√£ xong vi·ªác
        }
    )
    workflow.add_edge("tools", "agent") # c√≥ th·ªÉ g·ªçi tools ti·∫øp v√† add_conditional_edges l·∫°i ch·∫°y ƒë·ªÉ quy·∫øt ƒë·ªãnh
    # n·∫øu c·∫ßn tools th√¨ c·∫ßn ƒëi ti·∫øp ƒë·∫øn agent ƒë·ªÉ LLM tr√£ k·∫øt qu·∫£ theo ng√¥n ng·ªØ con ng∆∞·ªùi 
    
    # Compile
    if checkpointer:
        app = workflow.compile(checkpointer=checkpointer) # th·ª±c hi·ªán g√≥i workflow th√†nh executor (app)
        logger.info(f"‚úÖ Agent created with {len(TOOLS)} tools + PostgreSQL persistence")
    else:
        app = workflow.compile()
        logger.info(f"‚úÖ Agent created with {len(TOOLS)} tools (no persistence)")
    #app l√† executor ƒë·ªÉ g·ªçi agent v√† n√≥ ƒë∆∞·ª£c python g√≥i l·∫°i th√†nh m·ªôt Object
    return app, checkpointer 

# T·∫°o Global instance
agent_executor = None
checkpointer = None
# ƒë∆∞·ª£c g√°n ·ªü start_consumer()

# ==================== UTILITIES ====================
def user_shard_queue(user_id: str) -> str:
    """H√†m ph√¢n t√°n ng∆∞·ªùi d√πng v√†o m·ªôt queue c·ªë ƒë·ªãnh"""
    # gi√∫p l∆∞u l·∫°i context tr√™n m·ªói l·∫ßn h·ªôi tho·∫°i li√™n t·ª•c, tr√°nh query l·∫°i
    h = int(md5(user_id.encode()).hexdigest()[:8], 16) # hash user_id l·∫•y 8 k√Ω t·ª± ƒë·∫ßu v√† chuy·ªÉn th√†nh s·ªë nguy√™n
    return f"{config.AI_QUEUE_PREFIX}{h % config.SHARD_COUNT}"
    # n√™n c·∫£i ti·∫øn s·ª≠ d·ª•ng distributed state store l∆∞u cache l√™n redis (RAM) v√† Load Balancer ƒë·ªÉ ch·ªçn worker √≠t t·∫£i nh·∫•t
    # hi·ªán t·∫°i ch∆∞a c√≥ c∆° ch·∫ø m·ªói worker x·ª≠ l√Ω m·ªôt queue c·ªë ƒë·ªãnh, m√† c√°c worker ƒëang tranh nhau l·∫•y message n·∫øu r·∫£nh  
async def acquire_lock(redis: aioredis.Redis, key: str, ttl_ms: int = None) -> bool:
    """Lock request l·∫°i n·∫øu ƒë√£ c√≥ worker x·ª≠ l√Ω n√≥ (redis distributed lock)"""
    ttl = ttl_ms or config.LOCK_TTL_MS # kh√≥a 30s cho worker x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh)
    return await redis.set(key, "1", nx=True, px=ttl) #key l√† ƒë·∫∑t t√™n cho kh√≥a, v√† g√°n ƒë·∫°i value cho n√≥ l√† 1 
#nx = NotExists: ch·ªâ ƒë·∫∑t kh√≥a n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i , px th·ªùi gian 
# tr·∫£ v·ªÅ true n·∫øu ƒë·∫∑t kh√≥a th√†nh c√¥ng, false n·∫øu ƒë√£ c√≥ c√≥ worker kh√°c l·∫•y r·ªìi 
# n·∫øu h·∫øt th·ªùi gian redis t·ª± x√≥a v√† ch·ªù worker kh√°c l·∫•y 
async def release_lock(redis: aioredis.Redis, key: str):
    """X√≥a kh√≥a sau khi x·ª≠ l√Ω xong"""
    await redis.delete(key)
# aioredis l√† th∆∞ vi·ªán ch·∫°y redis b·∫•t ƒë·ªìng b·ªô
async def is_job_completed(redis: aioredis.Redis, job_id: str) -> bool:
    """Tr√°nh x·ª≠ l√Ω l·∫∑p l·∫°i c√¥ng vi·ªác ƒë√£ ho√†n th√†nh"""
    return await redis.sismember("jobs:completed", job_id)

async def mark_job_completed(redis: aioredis.Redis, job_id: str, ttl: int = 86400):
    """ƒê√°nh d·∫•u c√¥ng vi·ªác ƒë√£ ho√†n th√†nh ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh idempotency"""
    await redis.sadd("jobs:completed", job_id) # th√™m id job v√†o redis 
    await redis.expire("jobs:completed", ttl) # ƒë·∫∑t timeout 1 ng√†y 

# ==================== DATABASE OPERATIONS ====================
async def load_conversation_history(user_id: str, limit: int = None) -> List[Dict[str, str]]: # gi√° tr·ªã tr√£ v·ªÅ ( c√≥ hay kh√¥ng c≈©ng ƒëc v√¨ python l√† ng√¥n ng·ªØ dynamically typed)
    """l·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán t·ª´ PostgreSQL ( c√≥ gi·ªõi h·∫°n s·ªë tin nh·∫Øn l·∫•y)"""
    limit = limit or config.MAX_HISTORY_MESSAGES
    async with AsyncSessionLocal() as session: # t·∫°o ƒë·ªëi t∆∞·ª£ng session giao ti·∫øp v·ªõi db
        result = await session.execute(
            text("""
                SELECT role, content, created_at 
                FROM chat_messages 
                WHERE user_id = :user_id 
                ORDER BY created_at DESC 
                LIMIT :limit
            """),
            {"user_id": user_id, "limit": limit}
        ) # l·∫•y DESC tin nh·∫Øn t·ª´ d∆∞·ªõi l√™n tr√™n r·ªìi xu·ªëng kia m·ªõi reverse n√≥ l·∫°i 
        rows = result.fetchall() # l·∫•y t·∫•t c·∫£ c√°c d√≤ng k·∫øt qu·∫£
        return [
            {"role": row[0], "content": row[1], "created_at": str(row[2])} #row[0] l√† l·∫•y index c·ªôt 0 trong row ƒë√≥ 
            for row in reversed(rows) # l·∫∑p qua t·ª´ng row, ƒë·∫£o ng∆∞·ª£c th·ª© t·ª± ( t·ª©c l·∫∑p d∆∞·ªõi l√™n) ƒë·ªÉ c√≥ tin nh·∫Øn t·ª´ c≈© ƒë·∫øn m·ªõi
        ]

async def save_message(user_id: str, role: str, content: str):
    """L∆∞u tin nh·∫Øn v√†o PostgreSQL"""
    async with AsyncSessionLocal() as session:
        await session.execute(
            text("""
                INSERT INTO chat_messages (user_id, role, content, created_at)
                VALUES (:user_id, :role, :content, :created_at)
            """),
            {
                "user_id": user_id,
                "role": role,
                "content": content,
                "created_at": datetime.now() 
            } # ƒë√£ d√πng place holder ƒë·ªÉ tr√°nh SQL injection (:user_id, :role, ...)
        )
        await session.commit()

# ==================== AGENT EXECUTION ====================
async def invoke_agent(user_id: str, user_input: str, redis: aioredis.Redis) -> str:
    """
    Invoke LangGraph agent with memory and checkpointing
    Updated for LangGraph 1.0.0
    """
    try:
        # T·∫£i l·ªãch s·ª≠ chat
        history = await load_conversation_history(user_id)
        
        # chuy·ªÉn sang ƒë·ªãnh d·∫°ng m√† agent hi·ªÉu ƒë∆∞·ª£c
        messages = []
        for msg in history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
        
        # th√™m c√¢u input c·ªßa ng∆∞·ªùi d√πng v√†o lu√¥n 
        messages.append(HumanMessage(content=user_input))
        
        # Thread ID (gi·ªØ context h·ªôi tho·∫°i)
        thread_id = f"thread_{user_id}"
        
        # Configuration for agent
        config_dict = {
            "configurable": {
                "thread_id": thread_id
            }
        }
        
        # Invoke agent
        result = await asyncio.wait_for(
            agent_executor.ainvoke(
                {
                    "messages": messages,
                    "user_id": user_id,
                    "thread_id": thread_id
                },
                config=config_dict
            ),
            timeout=config.JOB_TIMEOUT_SEC
        )
        
        # Extract final response
        final_message = result["messages"][-1]
        response_text = final_message.content if hasattr(final_message, 'content') else str(final_message)
        
        logger.info(f"‚úÖ Agent response for user {user_id}: {response_text[:100]}...")
        return response_text
        
    except asyncio.TimeoutError:
        logger.error(f"‚è±Ô∏è Agent timeout for user {user_id}")
        return "Xin l·ªói, y√™u c·∫ßu c·ªßa b·∫°n m·∫•t qu√° nhi·ªÅu th·ªùi gian x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i."
    except Exception as e:
        logger.error(f"‚ùå Agent error for user {user_id}: {e}", exc_info=True)
        return "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n."

# ==================== MESSAGE HANDLER ====================
async def handle_message(
    redis: aioredis.Redis,
    payload: Dict[str, Any],
    message: aio_pika.IncomingMessage
):
    """Core message handler with idempotency and retries"""
    job_id = payload["job_id"]
    user_id = payload["user_id"]
    user_message = payload["text"]
    
    # 1. Idempotency check
    if await is_job_completed(redis, job_id):
        logger.info(f"‚è≠Ô∏è Job {job_id} already completed (idempotent)")
        await message.ack()
        return
    
    # 2. Acquire distributed lock
    lock_key = f"lock:job:{job_id}"
    if not await acquire_lock(redis, lock_key, config.LOCK_TTL_MS):
        logger.info(f"üîí Job {job_id} locked by another worker")
        await message.ack()
        return
    
    try:
        # 3. Double-check idempotency
        if await is_job_completed(redis, job_id):
            logger.info(f"‚è≠Ô∏è Job {job_id} completed during lock acquisition")
            await message.ack()
            return
        
        logger.info(f"üîÑ Processing job {job_id} for user {user_id}")
        
        # 4. Invoke agent
        reply = await invoke_agent(user_id, user_message, redis)
        
        # 5. Save to database
        await save_message(user_id, "user", user_message)
        await save_message(user_id, "assistant", reply)
        
        # 6. Mark as completed
        await mark_job_completed(redis, job_id)
        
        # 7. ACK message
        await message.ack()
        logger.info(f"‚úÖ Job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing job {job_id}: {e}", exc_info=True)
        
        # Retry logic
        headers = dict(message.headers) if message.headers else {}
        retries = int(headers.get("x-retries", 0))
        
        if retries < config.MAX_RETRIES:
            headers["x-retries"] = retries + 1
            headers["x-error"] = str(e)[:200]
            await message.nack(requeue=True)
            logger.warning(f"‚ö†Ô∏è Job {job_id} requeued (retry {retries + 1}/{config.MAX_RETRIES})")
        else:
            # Send to DLQ
            dlx_msg = aio_pika.Message(
                body=message.body,
                headers={**headers, "x-final-error": str(e)[:500]},
                delivery_mode=DeliveryMode.PERSISTENT
            )
            await message.channel.default_exchange.publish(
                dlx_msg,
                routing_key=config.DLQ_NAME
            )
            await message.ack()
            logger.error(f"‚ò†Ô∏è Job {job_id} sent to DLQ after {config.MAX_RETRIES} retries")
    
    finally:
        await release_lock(redis, lock_key)

# ==================== CONSUMER ====================
async def consume_shard(
    redis: aioredis.Redis,
    queue: aio_pika.Queue,
    shard_id: int,
    stop_event: asyncio.Event
):
    """Consume messages from a specific queue shard"""
    logger.info(f"üöÄ Worker shard-{shard_id} starting on queue '{queue.name}'")
    
    async def on_message(msg: aio_pika.IncomingMessage):
        async with msg.process(ignore_processed=True):
            try:
                payload = json.loads(msg.body.decode())
                logger.debug(f"[shard-{shard_id}] üì© Received: {payload}")
                await handle_message(redis, payload, msg)
            except json.JSONDecodeError as e:
                logger.error(f"[shard-{shard_id}] ‚ùå Invalid JSON: {e}")
                await msg.ack()
            except Exception as e:
                logger.error(f"[shard-{shard_id}] ‚ùå Handler error: {e}", exc_info=True)
                raise
    
    consumer_tag = await queue.consume(on_message, no_ack=False)
    logger.info(f"‚úÖ Shard-{shard_id} consumer registered (tag: {consumer_tag})")
    
    try:
        await stop_event.wait()
        logger.info(f"[shard-{shard_id}] üõë Stop signal received")
    except asyncio.CancelledError:
        logger.info(f"[shard-{shard_id}] üõë Task cancelled")
        raise
    finally:
        try:
            await queue.cancel(consumer_tag)
            logger.info(f"[shard-{shard_id}] ‚úÖ Consumer cancelled")
        except Exception as e:
            logger.warning(f"[shard-{shard_id}] ‚ö†Ô∏è Error cancelling consumer: {e}")

# ==================== INITIALIZATION ====================
async def init_infrastructure():
    """Initialize RabbitMQ, Redis, PostgreSQL connections"""
    try:
        logger.info("üîå Initializing infrastructure...")
        
        # RabbitMQ
        conn = await aio_pika.connect_robust(
            config.RABBITMQ_URL,
            timeout=10,
            client_properties={"connection_name": "ai_worker"}
        )
        channel = await conn.channel()
        await channel.set_qos(prefetch_count=config.PREFETCH_COUNT)
        
        # Declare DLX and DLQ
        dlx = await channel.declare_exchange(
            config.DLX_NAME,
            ExchangeType.DIRECT,
            durable=True
        )
        dlq = await channel.declare_queue(config.DLQ_NAME, durable=True)
        await dlq.bind(dlx, routing_key=config.DLQ_NAME)
        
        # Redis
        redis = await aioredis.from_url(
            config.REDIS_URL,
            decode_responses=True,
            max_connections=50
        )
        await redis.ping()
        
        logger.info("‚úÖ Infrastructure initialized: RabbitMQ, Redis, PostgreSQL")
        return conn, channel, redis
        
    except Exception as e:
        logger.error(f"‚ùå Infrastructure init failed: {e}", exc_info=True)
        raise

# ==================== MAIN CONSUMER ====================
async def start_consumer():
    """Main consumer loop with graceful shutdown"""
    global agent_executor, checkpointer
    
    # Initialize agent
    agent_executor, checkpointer = await create_agent_executor()
    
    # Initialize infrastructure
    conn, channel, redis = await init_infrastructure()
    
    # Stop event
    stop_event = asyncio.Event()
    
    def signal_handler(sig, frame):
        logger.info(f"üõë Received signal {sig}, initiating shutdown...")
        stop_event.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Create consumer tasks
    tasks = []
    for shard in range(config.SHARD_COUNT):
        queue_name = f"{config.AI_QUEUE_PREFIX}{shard}"
        queue = await channel.declare_queue(
            queue_name,
            durable=True,
            arguments={
                "x-dead-letter-exchange": config.DLX_NAME,
                "x-dead-letter-routing-key": config.DLQ_NAME
            }
        )
        task = asyncio.create_task(
            consume_shard(redis, queue, shard, stop_event)
        )
        tasks.append(task)
    
    logger.info(f"üéØ Started {len(tasks)} shard consumers")
    
    try:
        await asyncio.gather(*tasks, return_exceptions=True)
    except asyncio.CancelledError:
        logger.info("‚èπ Consumer tasks cancelled")
    finally:
        logger.info("üßπ Cleaning up connections...")
        await channel.close()
        await conn.close()
        await redis.aclose()
        if checkpointer:
            await checkpointer.aclose()
        logger.info("‚úÖ Shutdown complete")

# ==================== FASTAPI APP ====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üåü FastAPI starting up...")
    yield
    logger.info("üåô FastAPI shutting down...")

app = FastAPI(
    title="AI Message Gateway",
    description="Production-grade LangGraph 1.0.0 agent system",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "2.0.0",
        "langgraph": "1.0.0"
    }

@app.post("/send_message")
async def send_message(request: Request):
    """Enqueue user message for processing"""
    try:
        data = await request.json()
        user_id = data.get("user_id")
        text = data.get("message")
        
        if not user_id or not text:
            raise HTTPException(
                status_code=400,
                detail="Missing required fields: user_id, message"
            )
        
        conn, channel, redis = await init_infrastructure()
        
        try:
            job_id = str(uuid.uuid4())
            payload = {
                "job_id": job_id,
                "user_id": user_id,
                "text": text,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            queue_name = user_shard_queue(user_id)
            
            await channel.default_exchange.publish(
                aio_pika.Message(
                    body=json.dumps(payload).encode(),
                    delivery_mode=DeliveryMode.PERSISTENT,
                    content_type="application/json"
                ),
                routing_key=queue_name
            )
            
            logger.info(f"üì® Enqueued job {job_id} for user {user_id} to {queue_name}")
            
            return JSONResponse({
                "status": "ok",
                "job_id": job_id,
                "queue": queue_name,
                "message": "Job enqueued successfully"
            })
            
        finally:
            await channel.close()
            await conn.close()
            await redis.aclose()
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå API error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ==================== ENTRY POINT ====================
if __name__ == "__main__":
    try:
        logger.info("=" * 80)
        logger.info("üöÄ Starting AI Worker (LangGraph 1.0.0)")
        logger.info(f"   Shards: {config.SHARD_COUNT}")
        logger.info(f"   LLM: {config.LLM_MODEL}")
        logger.info(f"   Concurrency: {config.LLM_CONCURRENCY}")
        logger.info("=" * 80)
        
        asyncio.run(start_consumer())
        #loop = asyncio.SelectorEventLoop(selectors.SelectSelector())
        #asyncio.set_event_loop(loop)
        #loop.run_until_complete(start_consumer())
        #Windows m·∫∑c ƒë·ªãnh d√πng ProactorEventLoop, nh∆∞ng psycopg async kh√¥ng t∆∞∆°ng th√≠ch. √©p bu·ªôc d√πng SelectorEventLoop
    except KeyboardInterrupt:
        logger.info("üßπ Worker stopped by user")
    except Exception as e:
        logger.error(f"üî• Worker crashed: {e}", exc_info=True)
        sys.exit(1)