"""
üöÄ Production-Ready LangGraph AI Worker with RabbitMQ & Redis
Enterprise-grade distributed agent system for LangGraph 1.0.0

Author: Your Team
Version: 2.0.0-production (LangGraph 1.0.0)
"""

import asyncio
import json
import selectors
import uuid
import os
import logging
import sys
import signal
import httpx
from sse_starlette.sse import EventSourceResponse
from typing import Dict, Any, List, Optional, Annotated
from datetime import datetime
from hashlib import md5
from contextlib import suppress, asynccontextmanager
from datetime import datetime, timezone
# Message Broker & Cache
import aio_pika
from aio_pika import DeliveryMode, ExchangeType
import redis.asyncio as aioredis

# Web Framework
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Environment & Config
from dotenv import load_dotenv

# Database
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

# LangGraph & LangChain - UPDATED FOR 1.0.0
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition

# Conditional import for checkpointer
try:
    from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("‚ö†Ô∏è  PostgreSQL checkpointer not available. Running without persistence.")
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from typing_extensions import TypedDict

# Tools
from tools.register_tools import TOOLS

# ==================== CONFIGURATION ====================
load_dotenv()
#l·∫•y c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
class Config:
    """Centralized configuration with validation"""
    # Infrastructure
    RABBITMQ_URL: str = os.getenv("RABBITMQ_URL")
    REDIS_URL: str = os.getenv("REDIS_URL")
    DATABASE_URL: str = os.getenv("DATABASE_URL")
    CHECKPOINTER_DB_DSN: str = os.getenv("CHECKPOINTER_DB_DSN") 
    # Sharding
    SHARD_COUNT: int = int(os.getenv("SHARD_COUNT", "4"))
    AI_QUEUE_PREFIX: str = os.getenv("AI_QUEUE_PREFIX", "ai_jobs_")
    DLX_NAME: str = os.getenv("DLX", "ai_dlx")
    DLQ_NAME: str = "ai_jobs.dlq"
    SPRING_CHECK_TOKEN_URL : str = os.getenv("SPRING_CHECK_TOKEN_URL")
    
    # Performance
    LLM_CONCURRENCY: int = int(os.getenv("LLM_CONCURRENCY", "5"))
    PREFETCH_COUNT: int = int(os.getenv("PREFETCH_COUNT", "10"))
    DB_POOL_SIZE: int = int(os.getenv("DB_POOL_SIZE", "20"))
    DB_MAX_OVERFLOW: int = int(os.getenv("DB_MAX_OVERFLOW", "10"))
    
    # Retry & Timeout
    MAX_RETRIES: int = int(os.getenv("MAX_RETRIES", "5"))
    LOCK_TTL_MS: int = int(os.getenv("LOCK_TTL_MS", "30000"))
    JOB_TIMEOUT_SEC: int = int(os.getenv("JOB_TIMEOUT_SEC", "300"))
    
    # LLM
    USE_DUMMY: bool = os.getenv("USE_DUMMY", "false").lower() == "true"
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY")
    LLM_MODEL: str = os.getenv("LLM_MODEL", "gemini-2.0-flash-exp")
    LLM_TEMPERATURE: float = float(os.getenv("LLM_TEMPERATURE", "0.3"))
    LLM_MAX_TOKENS: int = int(os.getenv("LLM_MAX_TOKENS", "2048"))
    
    # Memory & History
    MAX_HISTORY_MESSAGES: int = int(os.getenv("MAX_HISTORY_MESSAGES", "20"))
    CHECKPOINT_TTL_SEC: int = int(os.getenv("CHECKPOINT_TTL_SEC", "3600"))
    
    # System Prompt t·ª´ file txt
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT: str = f.read()
    
    @classmethod
    def validate(cls):
        """Validate critical configuration"""
        required = ["RABBITMQ_URL", "REDIS_URL", "DATABASE_URL"]
        missing = [k for k in required if not getattr(cls, k)]
        # n·∫øu c√°c bi·∫øn m√¥i tr∆∞·ªùng quan tr·ªçng b·ªã thi·∫øu, n√©m l·ªói
        if missing:
            raise ValueError(f"Missing required config: {', '.join(missing)}")
        
        if not cls.USE_DUMMY and not cls.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY required when USE_DUMMY=false")

config = Config() #t·∫°o instance config
config.validate() #ki·ªÉm tra c·∫•u h√¨nh quan tr·ªçng tr∆∞·ªõc khi ch·∫°y

# ==================== LOGGING ====================
# c·∫•u h√¨nh cho h·ªá th·ªëng ghi log
logging.basicConfig( 
    level=logging.INFO, #ch·ªâ l·∫•y log t·ª´ m·ª©c INFO tr·ªü l√™n b·ªè qua DEBUG (WARNING, ERROR, CRITICAL)
    # gi·ªëng java LoggerFactory 
    format="%(asctime)s [%(levelname)s] [%(name)s:%(lineno)d] %(message)s",
    #asctime l√† th·ªùi gian | levelname l√† m·ª©c ƒë·ªô log (INFO, ...) | name l√† t√™n logger | lineno l√† s·ªë d√≤ng trong code | message l√† n·ªôi dung log
    datefmt="%Y-%m-%d %H:%M:%S", #ƒë·ªãnh d·∫°ng l·∫°i th·ªùi gian
    # 2025-10-23 17:33:05 v√≠ d·ª• 
    handlers=[
        logging.FileHandler("ai_worker.log", encoding="utf-8"), #ghi v√†o file
        logging.StreamHandler(sys.stdout) #ghi ra console
    ]
)
logger = logging.getLogger(__name__) #kh·ªüi t·∫°o logger trong file hi·ªán t·∫°i 

# ==================== DATABASE ====================
engine = create_async_engine( # s·ª≠ d·ª•ng k·∫øt n·ªëi b·∫•t ƒë·ªìng b·ªô ( kh√¥ng g√¢y ch·∫∑n ch∆∞∆°ng tr√¨nh khi ƒëang truy xu·∫•t)
    config.DATABASE_URL, # link k·∫øt n·ªëi db
    pool_size=config.DB_POOL_SIZE, #s·ªë k·∫øt n·ªëi t·ªëi ƒëa trong pool 
    # (gi·ªëng nh∆∞ c·ªïng ƒëi·ªán tho·∫°i c√¥ng c·ªông, ng∆∞·ªùi n√†y s·ª≠ d·ª•ng tr√£ l·∫°i ƒë·∫øn ng∆∞·ªùi kh√°c s·ª≠ d·ª•ng, c√≥ 20 c√°i ƒët)
    max_overflow=config.DB_MAX_OVERFLOW, #ph√≤ng tr∆∞·ªùng h·ª£p qu√° t·∫£i
    # (n·∫øu 20 c√°i ƒët ƒë·ªÅu b·∫≠n, c√≥ th·ªÉ t·∫°m th·ªùi m∆∞·ª£n th√™m 10 c√°i n·ªØa ƒë·ªÉ ph·ª•c v·ª• kh√°ch h√†ng)
    pool_pre_ping=True, #ki·ªÉm tra k·∫øt n·ªëi c√≤n s·ªëng tr∆∞·ªõc khi s·ª≠ d·ª•ng
    echo=False #kh√¥ng in c√¢u l·ªánh SQL ra log ( th·ª´a), ƒë·ªïi true n·∫øu mu·ªën xem chi ti·∫øt ƒë·ªÉ debug
)
AsyncSessionLocal = async_sessionmaker( #EntityManagerFactory trong java
    # t·∫°o session giao ti·∫øp b·∫•t ƒë·ªìng b·ªô v·ªõi db
    engine, # c·∫•u h√¨nh ·ªü tr√™n 
    class_=AsyncSession, #b·∫•t ƒë·ªìng b·ªô
    expire_on_commit=False # tr√°nh x√≥a d·ªØ li·ªáu trong session, ƒë·ªÉ truy xu·∫•t l·∫°i nhanh h∆°n
)

# ==================== LLM ====================
class DummyLLM: 
    """Dummy LLM d√πng ƒë·ªÉ test ƒë·ªÉ kh√¥ng t·ªën token"""
    async def ainvoke(self, messages):
        await asyncio.sleep(1) # tƒÉng ƒë·ªô tr·ªÖ
        last_msg = messages[-1] #l·∫•y tin nh·∫Øn cu·ªëi c√πng v√¨ trong ƒë·ªëi t∆∞·ª£ng messages s·∫Ω c√≥ nhi·ªÅu fiedls kh√°c nhau, fiedl cu·ªëi c√πng l√† humanMessage 
        content = last_msg.content if hasattr(last_msg, 'content') else str(last_msg) # in n·ªôi dung tin nh·∫Øn t·ª©c ph·∫ßn content, n·∫øu kh√¥ng th√¨ in h√™t ra tr√°nh l·ªói
        return AIMessage(content=f"[DUMMY] Response to: {content[:100]}...") # tr·∫£ v·ªÅ tin nh·∫Øn AIMessage v·ªõi n·ªôi dung gi·∫£ l·∫≠p
    
    def bind_tools(self, tools): # kh√¥ng g·∫Øn tools cho dummy 
        return self

# X√°c th·ª±c g·ªçi dummy ho·∫∑c Google Gemini
if config.USE_DUMMY:
    llm = DummyLLM()
    logger.info("üß™ Using DUMMY LLM (no API calls)")
else:
    llm = ChatGoogleGenerativeAI(
        model=config.LLM_MODEL, #g·ªçi model gemini c·ª• th·ªÉ 
        temperature=config.LLM_TEMPERATURE, # ƒë·ªô s√°ng t·∫°o c·ªßa c√¢u tr√£ l·ªùi (ƒë·ªÉ ·ªü m·ª©c an to√†n tr√°nh b·ªãa chuy·ªán)
        google_api_key=config.GOOGLE_API_KEY, #API key
        max_tokens=config.LLM_MAX_TOKENS, # gi·ªõi h·∫°n s·ªë token trong c√¢u tr·∫£ l·ªùi
        top_p=0.95, #ƒë·ªô r·ªông c·ªßa ph√¢n ph·ªëi x√°c su·∫•t (gi√∫p ƒëa d·∫°ng c√¢u tr·∫£ l·ªùi)
    )
    logger.info(f"ü§ñ Using Google Gemini: {config.LLM_MODEL}")

# g·∫Øn tools cho LLM gemini
llm_with_tools = llm.bind_tools(TOOLS) if not config.USE_DUMMY else llm
#TOOLS ƒë√£ ƒë∆∞·ª£c g·∫Øn s·∫µn v√† import 
# Semaphore for LLM concurrency control
llm_sem = asyncio.Semaphore(config.LLM_CONCURRENCY) # cho ph√©p s·ªë l∆∞·ª£ng LLM ƒë·ªìng th·ªùi m·ªôt l√∫c ch·∫°y l√† LLM_CONCURRENCY

# ==================== AGENT STATE (LANGGRAPH 1.0.0) ====================
from operator import add

class AgentState(TypedDict):
    """Agent state with job_id and redis"""
    messages: Annotated[List[BaseMessage], add]
    user_id: str
    thread_id: str
    jwt: str
    job_id: str # th√™m job_id ƒë·ªÉ tr·∫£ th√™m action cho UI

# ==================== AGENT NODES ====================
async def call_model(state: AgentState): #state ·ªü ƒë√¢y l√† dict gi·ªëng nh∆∞ Map trong java
    """Tr√°i tim c·ªßa agent - g·ªçi LLM v·ªõi messages ƒë√£ c√≥"""
    messages = state["messages"]
    user_id = state.get("user_id", "unknown")
    jwt = state.get("jwt", None)  # ‚úÖ L·∫•y JWT t·ª´ state
    job_id = state.get("job_id", None)  # ‚úÖ L·∫•y job_id t·ª´ state    
    # ‚úÖ Th√™m JWT v√†o system prompt ƒë·ªÉ LLM bi·∫øt
    system_content = config.SYSTEM_PROMPT
    if jwt:
        system_content += f"\n\nüîê **Authentication Context:**\nUser JWT Token (use this for API calls): `{jwt}`"
    if job_id:
        system_content += f"\n\nüîë **Job Context:**\nUser Job ID (use this for tracking): `{job_id}`"
    system_msg = SystemMessage(content=system_content)
    
    # Remove any existing system messages to avoid duplicates
    messages_without_system = [m for m in messages if not isinstance(m, SystemMessage)] 
    # x√≥a systemMessage c≈© n·∫øu c√≥, m s·∫Ω l∆∞u nh·ªØng ph·∫ßn c√≤n l·∫°i m√† kh√¥ng ph·∫£i systemMessage
    
    # ‚úÖ Filter out empty messages that could cause Gemini errors
    valid_messages = [] # danh s√°ch l∆∞u tr·ªØ c√°c message h·ª£p l·ªá ( l·ªçc c√°c message r·ªóng, tr√°nh l·ªói cho gemini)
    for m in messages_without_system:
        if hasattr(m, 'content') and m.content:
            valid_messages.append(m) #(hasattr(m, 'content') h√†m ki·ªÉm tra m c√≥ thu·ªôc t√≠nh content kh√¥ng, v√† m.content ki·ªÉm tra content c√≥ r·ªóng kh√¥ng)
        elif hasattr(m, 'tool_calls') and m.tool_calls:
            valid_messages.append(m)
        else:
            logger.warning(f"‚ö†Ô∏è Skipping empty message: {type(m).__name__}")
    
    # Add system prompt at the start
    messages_with_system = [system_msg] + valid_messages
    
    # b·∫Øt l·ªói n·∫øu ch·ªâ c√≥ m·ªói systemMessage m√† kh√¥ng c√≥ message n√†o kh√°c 
    if len(messages_with_system) < 2:
        logger.error(f"‚ùå Not enough messages to send to LLM: {len(messages_with_system)}") # s·ªë ph·∫ßn t·ª≠ trong messages_with_system
        return {"messages": [AIMessage(content="Xin l·ªói anh/ch·ªã, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu ·∫°.")]}
    
    # Log
    logger.debug(f"üîç Sending {len(messages_with_system)} messages to LLM")
    
    # Invoke LLM with tools
    try:
        async with llm_sem: #c√≤n ch·ªó tr·ªëng trong semaphore th√¨ m·ªõi g·ªçi LLM
            response = await llm_with_tools.ainvoke(messages_with_system) # g·ª≠i t·∫•t c·∫£ message ƒë√£ c√≥ (bao g·ªìm systemMessage v√† userMessage) ƒë·∫øn LLM ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        return {"messages": [response]} 
    except Exception as e:
        logger.error(f"‚ùå LLM invocation error: {e}")
        return {"messages": [AIMessage(content="Xin l·ªói anh/ch·ªã, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu ·∫°.")]}

def should_continue(state: AgentState):
    """Ki·ªÉm tra xem agent c√≥ c·∫ßn g·ªçi tools hay d·ª´ng l·∫°i"""
    messages = state["messages"] # l·∫•y tin nh·∫Øn 
    last_message = messages[-1] # l·∫•y tin nh·∫Øn cu·ªëi c√πng m√† AI v·ª´a t·∫°o ra 
    
    # ki·ªÉm tra xem n√≥ c√≥ mu·ªën g·ªçi tools hay kh√¥ng, kh√¥ng th√¨ cho n√≥ end 
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    # end state ·ªü ƒë√¢y
    return END

# ==================== LANGGRAPH AGENT (1.0.0) ====================
async def create_agent_executor():
    """
    Create LangGraph agent with PostgreSQL checkpointer
    Updated for LangGraph 1.0.0
    """
    # PostgreSQL checkpointer
    checkpointer = None
    checkpointer_cm = None  # L∆∞u context manager ƒë·ªÉ cleanup sau
    
    if POSTGRES_AVAILABLE and config.CHECKPOINTER_DB_DSN: #kh·ªüi t·∫°o checkpointer tƒÉng t√≠nh b·ªÅn v·ªØng
        try:
            # ‚úÖ FIX: from_conn_string() tr·∫£ v·ªÅ context manager, c·∫ßn await __aenter__()
            checkpointer_cm = AsyncPostgresSaver.from_conn_string(config.CHECKPOINTER_DB_DSN)
            checkpointer = await checkpointer_cm.__aenter__()
            # Gi·ªù checkpointer m·ªõi l√† AsyncPostgresSaver th·ª±c s·ª±
            await checkpointer.setup() # s·∫µn s√†ng s·ª≠ d·ª•ng 
            logger.info("‚úÖ PostgreSQL checkpointer enabled")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to setup PostgreSQL checkpointer: {e}")
            logger.info("üìù Running without persistence")
            checkpointer = None
            checkpointer_cm = None
    else:
        logger.info("üìù Running without checkpointer (no persistence)")
    
    # T·∫°o tool node 
    tool_node = ToolNode(TOOLS) 
    
    # ‚úÖ CRITICAL FIX: Wrap tool node to maintain conversation flow
    async def tool_node_with_context(state: AgentState):
        """
        Tool node wrapper t·ª± ƒë·ªông inject user_id v√† jwt v√†o tool calls
        """
        print("\n" + "=" * 80)
        print("üîß TOOL NODE WITH CONTEXT CALLED")
        
        messages = state['messages']
        user_id = state.get('user_id')
        jwt = state.get('jwt')
        job_id = state.get('job_id')
        
        print(f"üìä Context available:")
        print(f"   - user_id: {user_id}")
        print(f"   - jwt: {jwt[:20] if jwt else 'None'}...")
        print(f"   - job_id: {job_id}")
        
        # L·∫•y last message (AIMessage with tool_calls)
        last_message = messages[-1]
        
        if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
            print("‚ö†Ô∏è No tool calls found!")
            return {"messages": []}
        
        tool_messages = []
        
        for tool_call in last_message.tool_calls:
            tool_name = tool_call['name']
            tool_args = tool_call['args'].copy()  # Copy ƒë·ªÉ kh√¥ng modify original
            tool_id = tool_call['id']
            
            print(f"\nüõ†Ô∏è Processing tool: {tool_name}")
            print(f"   Original args: {tool_args}")
            
            # ‚úÖ INJECT CONTEXT v√†o tool args
            if tool_name == "create_booking":
                # Override user parameter v·ªõi user_id th·∫≠t t·ª´ state
                if user_id:
                    tool_args['user'] = user_id
                    print(f"   ‚úÖ Injected user_id: {user_id}")
                #if job_id:
                #    tool_args['job_id'] = job_id 
                else:
                    print(f"   ‚ö†Ô∏è No user_id in state!")
            
            print(f"   Final args: {tool_args}")
            
            # Execute tool v·ªõi args ƒë√£ inject
            try:
                from tools.register_tools import TOOLS
                
                # Find tool by name
                tool_func = None
                for t in TOOLS:
                    if t.name == tool_name:
                        tool_func = t
                        break
                
                if not tool_func:
                    result = json.dumps({
                        "error": f"Tool {tool_name} not found"
                    }, ensure_ascii=False)
                else:
                    # Call tool v·ªõi args ƒë√£ inject context
                    result = await tool_func.ainvoke(tool_args)
                
                print(f"   ‚úÖ Tool result: {str(result)[:100]}...")
                
            except Exception as e:
                print(f"   ‚ùå Tool error: {e}")
                result = json.dumps({
                    "error": str(e)
                }, ensure_ascii=False)
            
            # Create ToolMessage
            tool_messages.append(
                ToolMessage(
                    content=str(result),
                    tool_call_id=tool_id,
                    name=tool_name
                )
            )
        
        print(f"\nüîó Returning {len(tool_messages)} tool messages")
        print("=" * 80 + "\n")
        
        return {"messages": tool_messages}
    
    workflow = StateGraph(AgentState) # khai b√°o workflow v·ªõi state ƒë√£ fix
    #StateGraph ƒë·∫£m b·∫£o lu·ªìng c√¥ng vi·ªác c·ªßa agent ƒë∆∞·ª£c qu·∫£n l√Ω ƒë√∫ng c√°ch
    # Add nodes
    workflow.add_node("agent", call_model) #node n√†y g·ªçi LLM tr√£ v·ªÅ c√¢u tr·∫£ l·ªùi v√† toolCalls n·∫øu c√≥
    workflow.add_node("tools", tool_node_with_context) #node n√†y g·ªçi tools n·∫øu LLM y√™u c·∫ßu
  
    # b·∫Øt ƒë·∫ßu lu·ªìng c√¥ng vi·ªác
    workflow.add_edge(START, "agent") 
    # ki·ªÉm tra c√≥ toolMessage kh√¥ng ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·∫°y ti·∫øp hay d·ª´ng
    workflow.add_conditional_edges(
        "agent",
        should_continue, # check c√≥ toolMessage kh√¥ng
        {
            "tools": "tools",
            END: END # tr√£ end th√¨ d·ª´ng v√¨ l√∫c n√†y agent ƒë√£ xong vi·ªác
        }
    )
    workflow.add_edge("tools", "agent") # c√≥ th·ªÉ g·ªçi tools ti·∫øp v√† add_conditional_edges l·∫°i ch·∫°y ƒë·ªÉ quy·∫øt ƒë·ªãnh
    # n·∫øu c·∫ßn tools th√¨ c·∫ßn ƒëi ti·∫øp ƒë·∫øn agent ƒë·ªÉ LLM tr√£ k·∫øt qu·∫£ theo ng√¥n ng·ªØ con ng∆∞·ªùi 
    
    # Compile
    if checkpointer:
        app = workflow.compile(checkpointer=checkpointer) # th·ª±c hi·ªán g√≥i workflow th√†nh executor (app)
        logger.info(f"‚úÖ Agent created with {len(TOOLS)} tools + PostgreSQL persistence")
    else:
        app = workflow.compile()
        logger.info(f"‚úÖ Agent created with {len(TOOLS)} tools (no persistence)")
    #app l√† executor ƒë·ªÉ g·ªçi agent v√† n√≥ ƒë∆∞·ª£c python g√≥i l·∫°i th√†nh m·ªôt Object
    return app, checkpointer, checkpointer_cm  # Tr·∫£ v·ªÅ c·∫£ context manager

# T·∫°o Global instance
agent_executor = None
checkpointer = None # bi·∫øn n√†y thao t√°c ch√≠nh v·ªõi PostGre (th√™m x√≥a s·ª≠a)
checkpointer_cm = None  # Bi·∫øn to√†n c·ª•c cho context manager, d√πng ƒë·ªÉ m·ªü ƒë√≥ng k·∫øt n·ªëi n√™n c·∫ßn ƒë∆∞·ª£c g√°n l√† global (fix do h·ªá ƒë·ªÅu h√†nh window  )
# ƒë∆∞·ª£c g√°n ·ªü start_consumer()

# ==================== UTILITIES ====================
def user_shard_queue(user_id: str) -> str:
    """H√†m ph√¢n t√°n ng∆∞·ªùi d√πng v√†o m·ªôt queue c·ªë ƒë·ªãnh"""
    # gi√∫p l∆∞u l·∫°i context tr√™n m·ªói l·∫ßn h·ªôi tho·∫°i li√™n t·ª•c, tr√°nh query l·∫°i
    h = int(md5(user_id.encode()).hexdigest()[:8], 16) # hash user_id l·∫•y 8 k√Ω t·ª± ƒë·∫ßu v√† chuy·ªÉn th√†nh s·ªë nguy√™n
    return f"{config.AI_QUEUE_PREFIX}{h % config.SHARD_COUNT}"
    # n√™n c·∫£i ti·∫øn s·ª≠ d·ª•ng distributed state store l∆∞u cache l√™n redis (RAM) v√† Load Balancer ƒë·ªÉ ch·ªçn worker √≠t t·∫£i nh·∫•t
    # hi·ªán t·∫°i ch∆∞a c√≥ c∆° ch·∫ø m·ªói worker x·ª≠ l√Ω m·ªôt queue c·ªë ƒë·ªãnh, m√† c√°c worker ƒëang tranh nhau l·∫•y message n·∫øu r·∫£nh  
async def acquire_lock(redis: aioredis.Redis, key: str, ttl_ms: int = None) -> bool:
    """Lock request l·∫°i n·∫øu ƒë√£ c√≥ worker x·ª≠ l√Ω n√≥ (redis distributed lock)"""
    ttl = ttl_ms or config.LOCK_TTL_MS # kh√≥a 30s cho worker x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh)
    return await redis.set(key, "1", nx=True, px=ttl) #key l√† ƒë·∫∑t t√™n cho kh√≥a, v√† g√°n ƒë·∫°i value cho n√≥ l√† 1 
#nx = NotExists: ch·ªâ ƒë·∫∑t kh√≥a n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i , px th·ªùi gian 
# tr·∫£ v·ªÅ true n·∫øu ƒë·∫∑t kh√≥a th√†nh c√¥ng, false n·∫øu ƒë√£ c√≥ c√≥ worker kh√°c l·∫•y r·ªìi 
# n·∫øu h·∫øt th·ªùi gian redis t·ª± x√≥a v√† ch·ªù worker kh√°c l·∫•y 
async def release_lock(redis: aioredis.Redis, key: str):
    """X√≥a kh√≥a sau khi x·ª≠ l√Ω xong"""
    await redis.delete(key)
# aioredis l√† th∆∞ vi·ªán ch·∫°y redis b·∫•t ƒë·ªìng b·ªô
async def is_job_completed(redis: aioredis.Redis, job_id: str) -> bool:
    """Tr√°nh x·ª≠ l√Ω l·∫∑p l·∫°i c√¥ng vi·ªác ƒë√£ ho√†n th√†nh"""
    return await redis.sismember("jobs:completed", job_id)

async def mark_job_completed(redis: aioredis.Redis, job_id: str, ttl: int = 86400):
    """ƒê√°nh d·∫•u c√¥ng vi·ªác ƒë√£ ho√†n th√†nh ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh idempotency"""
    await redis.sadd("jobs:completed", job_id) # th√™m id job v√†o redis 
    await redis.expire("jobs:completed", ttl) # ƒë·∫∑t timeout 1 ng√†y 

# ==================== DATABASE OPERATIONS ====================
async def load_conversation_history(user_id: str, limit: int = None) -> List[Dict[str, str]]: # gi√° tr·ªã tr√£ v·ªÅ ( c√≥ hay kh√¥ng c≈©ng ƒëc v√¨ python l√† ng√¥n ng·ªØ dynamically typed)
    """l·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán t·ª´ PostgreSQL ( c√≥ gi·ªõi h·∫°n s·ªë tin nh·∫Øn l·∫•y)"""
    limit = limit or config.MAX_HISTORY_MESSAGES
    async with AsyncSessionLocal() as session: # t·∫°o ƒë·ªëi t∆∞·ª£ng session giao ti·∫øp v·ªõi db
        result = await session.execute(
            text("""
                SELECT role, content, created_at 
                FROM chat_messages 
                WHERE user_id = :user_id 
                ORDER BY created_at DESC 
                LIMIT :limit
            """),
            {"user_id": user_id, "limit": limit}
        ) # l·∫•y DESC tin nh·∫Øn t·ª´ d∆∞·ªõi l√™n tr√™n r·ªìi xu·ªëng kia m·ªõi reverse n√≥ l·∫°i 
        rows = result.fetchall() # l·∫•y t·∫•t c·∫£ c√°c d√≤ng k·∫øt qu·∫£
        return [
            {"role": row[0], "content": row[1], "created_at": str(row[2])} #row[0] l√† l·∫•y index c·ªôt 0 trong row ƒë√≥ 
            for row in reversed(rows) # l·∫∑p qua t·ª´ng row, ƒë·∫£o ng∆∞·ª£c th·ª© t·ª± ( t·ª©c l·∫∑p d∆∞·ªõi l√™n) ƒë·ªÉ c√≥ tin nh·∫Øn t·ª´ c≈© ƒë·∫øn m·ªõi
        ]

async def save_message(user_id: str, role: str, content: str):
    """L∆∞u tin nh·∫Øn v√†o PostgreSQL"""
    async with AsyncSessionLocal() as session:
        await session.execute(
            text("""
                INSERT INTO chat_messages (user_id, role, content, created_at)
                VALUES (:user_id, :role, :content, :created_at)
            """),
            {
                "user_id": user_id,
                "role": role,
                "content": content,
                "created_at": datetime.now() 
            } # ƒë√£ d√πng place holder ƒë·ªÉ tr√°nh SQL injection (:user_id, :role, ...)
        )
        await session.commit()

async def clear_checkpoint(user_id: str):
    """
    X√≥a checkpoint (tr·∫°ng th√°i h·ªôi tho·∫°i) c·ªßa ng∆∞·ªùi d√πng kh·ªèi Postgres checkpointer.
    ƒêi·ªÅu n√†y gi√∫p agent kh√¥ng reuse l·∫°i context c≈© cho l·∫ßn chat m·ªõi.
    """
    try:
        async with AsyncSessionLocal() as session:
            await session.execute(
                text("DELETE FROM checkpoints WHERE thread_id = :thread_id "),
                {"thread_id": f"thread_{user_id}"}
            )
            await session.execute(
                text("DELETE FROM checkpoint_blobs WHERE thread_id = :thread_id "),
                {"thread_id": f"thread_{user_id}"}

            )
            await session.execute(
                text("DELETE FROM checkpoint_writes WHERE thread_id = :thread_id "),
                {"thread_id": f"thread_{user_id}"}
            )       
            await session.commit()
        logger.info(f"‚úÖ Cleared checkpoint for {user_id}")
    except Exception as e:
        logger.error(f"‚ùå Failed to clear checkpoint for {user_id}: {e}", exc_info=True)
async def get_user_jwt(user_email: str, redis):
    """L·∫•y JWT t·ª´ Redis"""
    jwt = await redis.get(f"jwt:{user_email}")
    if jwt:
        return jwt
    print(f"‚ö†Ô∏è No JWT found for user email: {user_email}")
    return None
# ==================== AGENT EXECUTION ====================
async def invoke_agent(user_id: str, user_input: str, job_id: str, redis: aioredis.Redis) -> str:
    """
    G·ªçi agent c√πng v·ªõi checkpointer, memory l·ªãch s·ª≠ chat, Phi√™n l√†m vi·ªác c·ªßa 1 worker g·ªìm nhi·ªÅu node 
    (LangGraph 1.0.0)
    """
    try:
        # T·∫£i l·ªãch s·ª≠ chat
        history = await load_conversation_history(user_id, 100)
        
        # chuy·ªÉn sang ƒë·ªãnh d·∫°ng m√† agent hi·ªÉu ƒë∆∞·ª£c
        messages = []
        for msg in history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
        
        # th√™m c√¢u input c·ªßa ng∆∞·ªùi d√πng v√†o lu√¥n 
        messages.append(HumanMessage(content=user_input))
        
        # Thread ID (gi·ªØ context h·ªôi tho·∫°i)
        thread_id = f"thread_{user_id}"
        
        # Configuration for agent
        config_dict = {
            "configurable": {
                "thread_id": thread_id
            }
        }
        jwt = await get_user_jwt(user_id, redis)  # L·∫•y JWT t·ª´ Redis
        # g·ªçi agent 
        result = await asyncio.wait_for(
            agent_executor.ainvoke(
                {
                    "messages": messages,
                    "user_id": user_id,    
                    "thread_id": thread_id, # thread_id n√†y ƒë·ªÉ agent ƒë·ªçc l·∫°i context h·ªôi tho·∫°i
                    "jwt": jwt,
                    "job_id": job_id # truy·ªÅn job_id ƒë·ªÉ LLM bi·∫øt
                },
                config=config_dict # c·∫•u h√¨nh thread_id ƒë·ªÉ cho c√°c node trong agent d√πng chung, gi·ªØ context xuy√™n su·ªët workflow, ƒë·ªÉ tr√°nh ghi ƒë√® thread_id n·∫øu c√≥ nhi·ªÅu ng∆∞·ªùi d√πng c√πng h·ªôi tho·∫°i m·ªôt l√∫c
                # qu·∫£n l√Ω runtime, workflow l√† ch√≠nh 
            ),
            timeout=config.JOB_TIMEOUT_SEC # gi·ªõi h·∫°n th·ªùi gian agent x·ª≠ l√Ω 
        )
        
        # Extract final response
        final_message = result["messages"][-1]
        response_text = final_message.content if hasattr(final_message, 'content') else str(final_message)
        
        logger.info(f"‚úÖ Agent response for user {user_id}: {response_text[:100]}...")
        return response_text # tr·∫£ k·∫øt qu·∫£
        
    except asyncio.TimeoutError: # b·∫Øt l·ªói n·∫øu agent ch·∫°y qu√° th·ªùi gian 
        logger.error(f"‚è±Ô∏è Agent timeout for user {user_id}")
        return "Xin l·ªói, y√™u c·∫ßu c·ªßa anh/ch·ªã m·∫•t qu√° nhi·ªÅu th·ªùi gian x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i."
    except Exception as e:
        logger.error(f"‚ùå Agent error for user {user_id}: {e}", exc_info=True) # g·ª≠i ƒë·∫ßy ƒë·ªß th√¥ng tin log 
        return "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa anh/ch·ªã."

# ==================== MESSAGE HANDLER ====================
async def handle_message(
    redis: aioredis.Redis,
    payload: Dict[str, Any], # ƒëo·∫°n m√£ json ƒë∆∞·ª£c gi·∫£i m√£ th√†nh dict (hash map in java)
    message: aio_pika.IncomingMessage # giao ti·∫øp trung gian qua aio_pika, kh√¥ng th·ªÉ g·ªçi tr·ª±c ti·∫øp ƒë·∫øn RabbitMQ v√¨ kh√¥ng c√≥ th∆∞ vi·ªán 
):
    """B·ªô x·ª≠ l√Ω ch√≠nh cho m·ªói tin nh·∫Øn t·ª´ RabbitMQ, ƒë·∫£m b·∫£o idempotency v√† retry logic, DLQ"""
    job_id = payload["job_id"]
    user_id = payload["user_id"]
    user_message = payload["text"]
    
    # 1. Ki·ªÉm tra job ƒë√£ ho√†n th√†nh ch∆∞a 
    if await is_job_completed(redis, job_id):
        logger.info(f"‚è≠Ô∏è Job {job_id} already completed (idempotent)")
        await message.ack() # x√≥a kh·ªèi queue
        return
    
    # 2. Ki·ªÉm tra n·∫øu c√≥ worker kh√°c ƒë√£ l·∫•y
    lock_key = f"lock:job:{job_id}"
    if not await acquire_lock(redis, lock_key, config.LOCK_TTL_MS):
        logger.info(f"üîí Job {job_id} locked by another worker")
        await message.ack() 
        return
    
    try:
        # Ki·ªÉm tra l·∫°i
        if await is_job_completed(redis, job_id):
            logger.info(f"‚è≠Ô∏è Job {job_id} completed during lock acquisition")
            await message.ack()
            return
        
        logger.info(f"üîÑ Processing job {job_id} for user {user_id}")
        
        # G·ªçi agent
        reply = await invoke_agent(user_id, user_message, job_id, redis)

        # L∆∞u v√†o c∆° s·ªü d·ªØ li·ªáu
        await save_message(user_id, "user", user_message)
        await save_message(user_id, "assistant", reply)

        # ƒê√°nh d·∫•u l√† ƒë√£ ho√†n th√†nh
        await mark_job_completed(redis, job_id)
        # ACK message
        await message.ack()
        logger.info(f"‚úÖ Job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing job {job_id}: {e}", exc_info=True)
        # khi c√≥ l·ªói x·∫£y ra th·ª≠ l·∫°i 
        # Retry logic
        headers = dict(message.headers) if message.headers else {} #l·∫•y header t·ª´ rabbitmq message, ƒë·ªÉ l∆∞u s·ªë l·∫ßn th·ª≠ l·∫°i 
        retries = int(headers.get("x-retries", 0)) # l·∫•y n·∫øu c√≥ ho·∫∑c g√°n b·∫±ng 0 
        
        if retries < config.MAX_RETRIES: # ch·ªâ th·ª≠ l·∫°i s·ªë l·∫ßn c√≥ h·∫°n 
            headers["x-retries"] = retries + 1
            headers["x-error"] = str(e)[:200]
            await message.nack(requeue=True)
            logger.warning(f"‚ö†Ô∏è Job {job_id} requeued (retry {retries + 1}/{config.MAX_RETRIES})")
        else:
            # chuy·ªÉn v√†o Dead Letter Queue n·∫øu v∆∞·ª£t qu√° s·ªë l·∫ßn th·ª≠, n∆°i l∆∞u tin nh·∫Øn b·ªã l·ªói 
            dlx_msg = aio_pika.Message(
                body=message.body,
                headers={**headers, "x-final-error": str(e)[:500]},
                delivery_mode=DeliveryMode.PERSISTENT # Persistent l∆∞u tr√™n ·ªï ƒëƒ©a b·ªÅn v·ªØng, Transient l∆∞u trong RAM  
            )
            # g·ª≠i ƒë·∫øn channel .default_exchange l√† n∆°i ph√¢n ph·ªëi tin nh·∫Øn c·ªßa rabbitmq .publish ch·ªçn g·ª≠i, tin nh·∫Øn s·∫Ω ch·∫°y ƒë·∫øn key ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a 
            await message.channel.default_exchange.publish(
                dlx_msg,
                routing_key=config.DLQ_NAME
            )
            await message.ack() # b·∫•t l·ª±c ack n√≥ ra 
            logger.error(f"‚ò†Ô∏è Job {job_id} sent to DLQ after {config.MAX_RETRIES} retries")
    finally:
        await release_lock(redis, lock_key) # gi·∫£i ph√≥ng kh√≥a trong redis d√π th√†nh c√¥ng hay th·∫•t b·∫°i
        await clear_checkpoint(user_id) # x√≥a checkpoint ƒë·ªÉ tr√°nh reuse context c≈©
# ==================== CONSUMER ====================
async def consume_shard(
    redis: aioredis.Redis,
    queue: aio_pika.Queue,
    shard_id: int,
    stop_event: asyncio.Event
):
    """ƒêƒÉng k√Ω consumer cho m·ªói shard (queue), v√† g·ªçi handle_message x·ª≠ l√Ω tin nh·∫Øn"""
    logger.info(f"üöÄ Worker shard-{shard_id} starting on queue '{queue.name}'")
    
    async def on_message(msg: aio_pika.IncomingMessage): # truy·ªÅn v√†o ƒë·ªëi t∆∞·ª£ng msg t·ª´ rabbitmq
        async with msg.process(ignore_processed=True): # t·ª± ƒë·ªông ack sau khi x·ª≠ l√Ω xong, ignore_processed tr√°nh l·ªói n·∫øu ƒë√£ ack r·ªìi
            try:
                payload = json.loads(msg.body.decode()) # gi·∫£i m√£ json th√†nh dict
                logger.debug(f"[shard-{shard_id}] üì© Received: {payload}")
                await handle_message(redis, payload, msg) # g·ª≠i cho agent x·ª≠ l√Ω
            except json.JSONDecodeError as e:
                logger.error(f"[shard-{shard_id}] ‚ùå Invalid JSON: {e}")
                await msg.ack()
            except Exception as e:
                logger.error(f"[shard-{shard_id}] ‚ùå Handler error: {e}", exc_info=True)
                raise # n√©m l·ªói ra h√†m msg.process ƒë·ªÉ x·ª≠ l√Ω retry v√† DLQ
    
    consumer_tag = await queue.consume(on_message, no_ack=False) # ƒëƒÉng k√Ω t·ª± ƒë·ªông g·ªçi h√†m on_message khi c√≥ tin nh·∫Øn m·ªõi t·ª´ queue, no_ack = false ƒë·ªÉ worker t·ª± ack sau khi x·ª≠ l√Ω xong
    # on_message l√† callback function c·ªßa th∆∞ vi·ªán aio-pika, th∆∞ vi·ªán s·∫Ω t·ª± truy·ªÅn tham s·ªë 
    #no_ack=False ch·ªù (msg.process(ignore_processed=True) ack, ho·∫∑c h√†m handle_message ack) ƒë·ªÉ true rabbitMQ giao r·ªìi x√≥a lu√¥n kh·ªèi queue
    logger.info(f"‚úÖ Shard-{shard_id} consumer registered (tag: {consumer_tag})")
    
    # d·ª´ng khi c√≥ t√≠n hi·ªáu
    try:
        await stop_event.wait()
        logger.info(f"[shard-{shard_id}] üõë Stop signal received")
    except asyncio.CancelledError:
        logger.info(f"[shard-{shard_id}] üõë Task cancelled")
        raise
    finally:
        try:
            await queue.cancel(consumer_tag)
            logger.info(f"[shard-{shard_id}] ‚úÖ Consumer cancelled")
        except Exception as e:
            logger.warning(f"[shard-{shard_id}] ‚ö†Ô∏è Error cancelling consumer: {e}")

# ==================== INITIALIZATION ====================
async def init_infrastructure():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi RabbitMQ, Redis"""
    try:
        logger.info("üîå Initializing infrastructure...")
        
        # RabbitMQ
        conn = await aio_pika.connect_robust( # aio_pika.connect_robust() k·∫øt n·ªëi l·∫°i t·ª± ƒë·ªông n·∫øu m·∫•t k·∫øt n·ªëi
            config.RABBITMQ_URL,
            timeout=10,
            client_properties={"connection_name": "ai_worker"}
        )
        # t·∫°o channel nh·ªè trong k·∫øt n·ªëi l·ªõn conn, ƒë·ªß s·ª≠ d·ª•ng v√† ti·∫øt ki·ªám t√†i nguy√™n 
        channel = await conn.channel() #t·∫°o chanel trong conn, ƒë∆∞·ªùng ·ªëng nh·ªè 
        await channel.set_qos(prefetch_count=config.PREFETCH_COUNT) # thi·∫øt l·∫≠p s·ªë l∆∞·ª£ng message t·ªëi ƒëa m√† worker c√≥ th·ªÉ l·∫•y c√πng l√∫c, gi√∫p ph√¢n t√°n message ƒë·ªÅu cho c√°c worker
        # m·ªói worker s·∫Ω ƒë∆∞·ª£c rabbitMQ r√≥t cho 10 message nh∆∞ng khi x·ª≠ l√Ω th√¨ m·ªõi lock message ƒë√≥ ( t·ª©c l√† n·∫øu worker kh√°c xong c≈©ng c√≥ th·ªÉ l·∫•y message c·ªßa worker kia x·ª≠ l√Ω ti·∫øp)
        
        # Declare DLX and DLQ
        dlx = await channel.declare_exchange( # b∆∞u ƒëi·ªán nh·∫≠n tin nh·∫Øn l·ªói
            config.DLX_NAME, # t√™n DLX
            ExchangeType.DIRECT, # ki·ªÉu direct ƒë·ªÉ routing key kh·ªõp m·ªõi g·ª≠i ƒë·∫øn queue
            durable=True # t·ªìn t·∫°i l√¢u d√†i 
        )
        dlq = await channel.declare_queue(config.DLQ_NAME, durable=True) # t·∫°o n∆°i nh·∫≠n tin nh·∫Øn ch·∫øt (dead letter queue)
        await dlq.bind(dlx, routing_key=config.DLQ_NAME) # g√°n ƒë·ªãa ch·ªâ cho b∆∞u ƒëi·ªán DLX giao tin nh·∫Øn ch·∫øt ƒë·∫øn DLQ
        
        # Redis
        redis = await aioredis.from_url(
            config.REDIS_URL,
            decode_responses=True,
            max_connections=50
        )
        await redis.ping()

        logger.info("‚úÖ Infrastructure initialized: RabbitMQ, Redis")
        return conn, channel, redis
        
    except Exception as e:
        logger.error(f"‚ùå Infrastructure init failed: {e}", exc_info=True)
        raise

# ==================== MAIN CONSUMER ====================
async def start_consumer():
    """Main consumer loop with graceful shutdown"""
    global agent_executor, checkpointer, checkpointer_cm  
    
    # ch·ª©a workflow , thao t√°c v·ªõi db, c·ªïng k·∫øt n·ªëi
    agent_executor, checkpointer, checkpointer_cm = await create_agent_executor() 
    
    # Initialize c√°c k·∫øt n·ªëi 
    conn, channel, redis = await init_infrastructure()
    
    # Stop event
    stop_event = asyncio.Event()
    
    def signal_handler(sig, frame):
        logger.info(f"üõë Received signal {sig}, initiating shutdown...")
        stop_event.set()
    # sig m√£ t√≠n hi·ªáu SIGINT = 2, SIGTERM = 15| b·∫Øt bu·ªôc ph·∫£i truy·ªÅn ƒë·ªß tham s·ªë sig, frame t∆∞∆°ng th√≠ch v·ªõi th∆∞ vi·ªán signal c·ªßa python
    signal.signal(signal.SIGINT, signal_handler) # b·∫Øt t√≠n hi·ªáu ctrl+c ƒë·ªÉ d·ª´ng
    signal.signal(signal.SIGTERM, signal_handler) # b·∫Øt t√≠n hi·ªáu d·ª´ng t·ª´ h·ªá ƒëi·ªÅu h√†nh ( docker, ho·∫∑c h·ªá ƒëi·ªÅu h√†nh g·ªçi kill)
    #signal_handler l√† callback function
    # T·∫°o worker cho m·ªói shard (consumers)
    tasks = [] # gi·ªè ƒë·ª±ng c√¥ng vi·ªác 
    for shard in range(config.SHARD_COUNT): # t·∫°o ra 8 task cho m·ªói worker (worker l√† m·ªôt l·∫ßn python main.py, c√≥ th·ªÉ t·∫°o nhi·ªÅu worker b·∫±ng docker) 
        queue_name = f"{config.AI_QUEUE_PREFIX}{shard}"
        queue = await channel.declare_queue( # t·∫°o 8 queue nh∆∞ ƒë√£ ƒë·ªãnh nghƒ©a trong config n·∫øu ch∆∞a c√≥ 
            queue_name,
            durable=True,
            arguments={
                "x-dead-letter-exchange": config.DLX_NAME,
                "x-dead-letter-routing-key": config.DLQ_NAME
            }
        )
        task = asyncio.create_task( # t·∫°o 8 task ch·∫°y song song( tr√™n 1 worker )
            consume_shard(redis, queue, shard, stop_event) # m·ªói task l·∫Øng nghe m·ªôt queue c·ªë ƒë·ªãnh
        )
        tasks.append(task)
    
    logger.info(f"üéØ Started {len(tasks)} shard consumers")
    
    try:
        await asyncio.gather(*tasks, return_exceptions=True) #return_exceptions=True n·∫øu 1 task b·ªã l·ªói th√¨ c√°c task kh√°c v·∫´n ch·∫°y ti·∫øp
        # c√¥ng d·ª•ng asyncio.gather() ch·∫°y nhi·ªÅu task c√πng l√∫c, v√† gi·ªØ nguy√™n ch∆∞∆°ng tr√¨nh, ch·ªâ k·∫øt th√∫c v√† end task khi c√≥ t√≠n hi·ªáu d·ª´ng
    except asyncio.CancelledError:
        logger.info("X Consumer tasks cancelled")
    finally:
        logger.info("üßπ Cleaning up connections...")
        await channel.close()
        await conn.close()
        await redis.aclose()
        # ƒê√≥ng checkpointer ƒë√∫ng c√°ch qua context manager
        if checkpointer_cm:
            try:
                await checkpointer_cm.__aexit__(None, None, None)
                logger.info("‚úÖ Checkpointer closed")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error closing checkpointer: {e}")
        logger.info("‚úÖ Shutdown complete")
# ==================== FASTAPI APP ====================
@asynccontextmanager # ƒë·ªãnh nghƒ©a h√†m b·∫•t ƒë·ªìng b·ªô d√πng l√†m context manager ( ng∆∞·ªùi gi·ªØ c·ª≠a k·∫øt n·ªëi)
async def lifespan(app: FastAPI):
    logger.info("üåü FastAPI starting up...") # d√≤ng n√†y s·∫Ω ch·∫°y khi kh·ªüi ƒë·ªông v√¨ n·∫±m tr∆∞·ªõc yield
    yield #app ch·∫°y ·ªü ƒë√¢y v√† gi·ªØ ch·ªù tin hi·ªáu t·∫Øt m·ªõi ch·∫°y d√≤ng d∆∞·ªõi 
    logger.info("üåô FastAPI shutting down...") 

app = FastAPI( # t·∫°o k·∫øt n·ªëi FastAPI
    title="AI Message Gateway", # c√°c th√¥ng tin n√†y hi·ªán l√™n UI swagger
    description="Production-grade LangGraph 1.0.0 agent system",
    version="2.0.0",
    lifespan=lifespan # g√°n context manager
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Cho ph√©p T·∫§T C·∫¢ website g·ªçi 
    #"https://abdcjddahdaj.com",   ‚Üê Ch·ªâ ri√™ng domain n√†y
    #"http://abdcjddahdaj.com"     ‚Üê N·∫øu c·∫ßn c·∫£ HTTP

    allow_credentials=True, # Cho ph√©p g·ª≠i cookie/token  
    allow_methods=["*"], # Cho ph√©p T·∫§T C·∫¢ method (GET, POST, PUT...)
    allow_headers=["*"], # Cho ph√©p T·∫§T C·∫¢ headers
)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": "2.0.0",
        "langgraph": "1.0.0"
    }

@app.post("/send_message")
async def send_message(request: Request): # nh·∫≠n to√†n b·ªô request t·ª´ client
    """Enqueue user message for processing"""
    try:
        cookie_jar = request.cookies
        jwt_token = cookie_jar.get("jwt")
        if not jwt_token:
            raise HTTPException(status_code=401, detail="authentication failed")       
        async with httpx.AsyncClient() as client:
            response = await client.post(
                config.SPRING_CHECK_TOKEN_URL,
                cookies={"jwt": jwt_token}
            )
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="authentication failed")
        else:
            userName = response.text.strip()

            data = await request.json()
            user_id = userName
            text = data.get("message")
            
            if not text:
                raise HTTPException(
                    status_code=400,
                    detail="Missing required fields: message"
                )
            
            conn, channel, redis = await init_infrastructure()
            
            try:
                # set JWT v√†o redis ƒë·ªÉ worker s·ª≠ d·ª•ng g·ªçi API BE 
                await redis.setex(
                    f"jwt:{userName}", 
                    600, 
                    jwt_token
                )
                job_id = str(uuid.uuid4())
                payload = {
                    "job_id": job_id,
                    "user_id": user_id,
                    "text": text,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                queue_name = user_shard_queue(user_id) # ph√¢n t√°n v√†o m·ªôt queue c·ªë ƒë·ªãnh
                
                await channel.default_exchange.publish(
                    aio_pika.Message( # t·∫°o object message ƒë·ªÉ g·ª≠i
                        body=json.dumps(payload).encode(), #chuy·ªÉn dict th√†nh json r·ªìi m√£ h√≥a th√†nh bytes v√¨ rabbitmq ch·ªâ nh·∫≠n ƒëc d·ªØ li·ªáu bytes
                        delivery_mode=DeliveryMode.PERSISTENT, # l∆∞u b·ªÅn v·ªØng v√†o ·ªï ƒëƒ©a
                        content_type="application/json"
                    ),
                    routing_key=queue_name # g·ª≠i ƒë·∫øn queue ƒë√£ ph√¢n shard ·ªü tr√™n
                )
                
                logger.info(f"üì® Enqueued job {job_id} for user {user_id} to {queue_name}")
                
                return JSONResponse({
                    "status": "ok",
                    "job_id": job_id,
                    "queue": queue_name,
                    "message": "Job enqueued successfully"
                })
                
            finally: # ƒë√≥ng k·∫øt n·ªëi, d√π th√†nh c√¥ng hay l·ªói, m·ªói request s·∫Ω ƒë·ªÅu t·∫°o connect m·ªõi v√† ƒë√≥ng, tr√°nh gi·ªØ k·∫øt n·ªëi l√¢u t·ªën t√†i nguy√™n
                await channel.close()
                await conn.close()
                await redis.aclose()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå API error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stream/{job_id}")
async def stream_result(job_id: str, request: Request):
    """
    SSE endpoint - Stream k·∫øt qu·∫£ real-time t·ª´ AI worker
    Client s·∫Ω nh·∫≠n events li√™n t·ª•c cho ƒë·∫øn khi job ho√†n th√†nh
    """
    try:
        # ‚úÖ 1. X√°c th·ª±c JWT
        cookie_jar = request.cookies
        jwt_token = cookie_jar.get("jwt")
        
        if not jwt_token:
            raise HTTPException(status_code=401, detail="Missing authentication token")
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                config.SPRING_CHECK_TOKEN_URL,
                cookies={"jwt": jwt_token}
            )
        
        if response.status_code != 200:
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        
        user_email = response.text.strip()
        logger.info(f"üì° SSE stream started for job {job_id} (user: {user_email})")
        
        # ‚úÖ 2. Generator function ƒë·ªÉ stream events
        async def event_generator():
            redis = await aioredis.from_url(config.REDIS_URL, decode_responses=True)
            
            try:
                max_attempts = 120  # T·ªëi ƒëa 2 ph√∫t (120 gi√¢y)
                attempt = 0
                
                while attempt < max_attempts:
                    # ‚úÖ Ki·ªÉm tra client c√≤n k·∫øt n·ªëi kh√¥ng
                    if await request.is_disconnected():
                        logger.info(f"üîå Client disconnected for job {job_id}")
                        break
                    
                    # ‚úÖ Ki·ªÉm tra job ƒë√£ ho√†n th√†nh ch∆∞a
                    is_completed = await redis.sismember("jobs:completed", job_id)
                    
                    if is_completed:
                        # Job ƒë√£ xong - L·∫•y k·∫øt qu·∫£
                        result = await redis.get(f"job:{job_id}:result")
                        
                        if not result:
                            # Fallback: L·∫•y t·ª´ database n·∫øu Redis kh√¥ng c√≥
                            async with AsyncSessionLocal() as session:
                                db_result = await session.execute(
                                    text("""
                                        SELECT content, created_at
                                        FROM chat_messages 
                                        WHERE user_id = :user_id 
                                        AND role = 'assistant'
                                        ORDER BY created_at DESC 
                                        LIMIT 1
                                    """),
                                    {"user_id": user_email}
                                )
                                row = db_result.fetchone()
                                result = row[0] if row else "L·ªói: Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"
                        # L·∫•y action n·∫øu c√≥
                        action = "none"
                        actionId = "none"
                        rank = "none"
                        if(await redis.exists(job_id)):
                            action = await redis.hget(job_id, "action")
                            actionId = await redis.hget(job_id, "idAction")
                            rank = await redis.hget(job_id, "rank")
                            await redis.delete(job_id) 
                        # ‚úÖ G·ª¨I K·∫æT QU·∫¢ CU·ªêI C√ôNG
                        logger.info(f"‚úÖ Sending final result for job {job_id}")
                        yield {
                            "event": "message",
                            "data": json.dumps({
                                "status": "completed",
                                "job_id": job_id,
                                "result": result,
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "action": action,
                                "actionId": actionId,
                                "rank": rank
                            }, ensure_ascii=False)
                        }
                        
                        # G·ª≠i event ƒë√≥ng k·∫øt n·ªëi
                        yield {
                            "event": "done",
                            "data": json.dumps({"status": "stream_ended"})
                        }
                        break
                    
                    # ‚úÖ G·ª¨I HEARTBEAT m·ªói 3 gi√¢y ƒë·ªÉ gi·ªØ k·∫øt n·ªëi
                    if attempt % 3 == 0:
                        logger.debug(f"üíì Heartbeat for job {job_id} (attempt {attempt})")
                        yield {
                            "event": "heartbeat",
                            "data": json.dumps({
                                "status": "processing",
                                "job_id": job_id,
                                "attempt": attempt,
                                "message": "ƒêang x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n...",
                                "action": "processing"
                            })
                        }
                    
                    attempt += 1
                    await asyncio.sleep(1)  # Poll m·ªói gi√¢y
                
                # ‚úÖ TIMEOUT n·∫øu qu√° l√¢u
                if attempt >= max_attempts:
                    logger.warning(f"‚è±Ô∏è Job {job_id} timeout after {max_attempts}s")
                    yield {
                        "event": "error",
                        "data": json.dumps({
                            "status": "timeout",
                            "job_id": job_id,
                            "message": "Y√™u c·∫ßu x·ª≠ l√Ω qu√° l√¢u. Vui l√≤ng th·ª≠ l·∫°i sau."
                        })
                    }
            
            except Exception as e:
                logger.error(f"‚ùå SSE generator error for job {job_id}: {e}", exc_info=True)
                yield {
                    "event": "error",
                    "data": json.dumps({
                        "status": "error",
                        "message": str(e)
                    })
                }
            
            finally:
                await redis.aclose()
                logger.info(f"üîö SSE stream ended for job {job_id}")
        
        # ‚úÖ Tr·∫£ v·ªÅ SSE response
        return EventSourceResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no"  # T·∫Øt buffering cho nginx
            }
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå SSE endpoint error for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
@app.post("/update_location")
async def update_location(request: Request):
    """
    C·∫≠p nh·∫≠t v·ªã tr√≠ GPS c·ªßa user (latitude, longitude)
    Frontend g·ªçi API n√†y m·ªói khi user di chuy·ªÉn >= 500m
    
    Request body:
    {
        "latitude": 10.762622,
        "longitude": 106.660172
    }
    """
    try:
        # ‚úÖ 1. X√°c th·ª±c JWT
        cookie_jar = request.cookies
        jwt_token = cookie_jar.get("jwt")
        
        if not jwt_token:
            raise HTTPException(status_code=401, detail="Missing authentication token")
        
        # Verify token v·ªõi Spring backend
        async with httpx.AsyncClient() as client:
            response = await client.post(
                config.SPRING_CHECK_TOKEN_URL,
                cookies={"jwt": jwt_token}
            )
        
        if response.status_code != 200:
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        
        user_email = response.text.strip()
        
        # ‚úÖ 2. Parse v√† validate coordinates
        data = await request.json()
        latitude = data.get("latitude")
        longitude = data.get("longitude")
        print("‚úÖ‚úÖ‚úÖLatitude:", latitude, "‚úÖ‚úÖ‚úÖLongitude:", longitude)
        if latitude is None or longitude is None:
            raise HTTPException(
                status_code=400,
                detail="Missing required fields: latitude, longitude"
            )
        
        try:
            lat = float(latitude)
            lon = float(longitude)
            
            if not (-90 <= lat <= 90):
                raise ValueError("Latitude must be between -90 and 90")
            if not (-180 <= lon <= 180):
                raise ValueError("Longitude must be between -180 and 180")
        except ValueError as e:
            raise HTTPException(status_code=400, detail=f"Invalid coordinates: {str(e)}")
        
        # ‚úÖ 3. L∆∞u v√†o Redis
        redis = await aioredis.from_url(config.REDIS_URL, decode_responses=True)
        
        try:
            location_key = f"location:{user_email}"
            location_data = {
                "latitude": lat,
                "longitude": lon,
                "updated_at": datetime.now(timezone.utc).isoformat()
            }
            
            # L∆∞u v·ªõi TTL 24 gi·ªù
            await redis.setex(
                location_key,
                86400,  # 24 hours
                json.dumps(location_data)
            )
            
            logger.info(f"üìç Updated location for {user_email}: ({lat}, {lon})")
            
            return JSONResponse({
                "status": "success",
                "message": "Location updated successfully",
                "data": {
                    "latitude": lat,
                    "longitude": lon,
                    "timestamp": location_data["updated_at"]
                }
            })
        
        finally:
            await redis.aclose()
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Update location error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# ==================== DETECT CHARGING TYPE ENDPOINT ====================
@app.post("/api/detect-charging-type")
async def detect_charging_type_endpoint(request: Request):
    """
    API endpoint ƒë·ªÉ ph√°t hi·ªán lo·∫°i s·∫°c xe ƒëi·ªán d·ª±a tr√™n t√™n xe
    """
    try:
        # ‚úÖ Parse request body
        data = await request.json()
        car_name = data.get("car_name", "").strip()
        
        if not car_name:
            raise HTTPException(
                status_code=400,
                detail="Missing required field: car_name"
            )
        
        if len(car_name) < 3:
            raise HTTPException(
                status_code=400,
                detail="Car name too short. Please provide full name (e.g., 'VinFast VF5')"
            )
        
        logger.info(f"üîç Detecting charging type for: {car_name}")
        
        # ‚úÖ Import function t·ª´ API_BE
        from tools.API_BE import detect_charging_type_by_car_name
        
        # ‚úÖ G·ªçi function detect
        result = await detect_charging_type_by_car_name(car_name)
        
        logger.info(f"‚úÖ Detected: {result['charging_type']} (confidence: {result['confidence']})")
        
        return JSONResponse({
            "status": "success",
            "data": result
        })
    
    except HTTPException:
        raise
    
    except Exception as e:
        logger.error(f"‚ùå Detect charging type error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to detect charging type: {str(e)}"
        )
    
# ==================== ENTRY POINT ====================
if __name__ == "__main__": # ch·ªâ ch·∫°y ƒë∆∞·ª£c khi run file n√†y tr·ª±c ti·∫øp, kh√¥ng ch·∫°y ƒë∆∞·ª£c khi import
    try:
        logger.info("=" * 80)
        logger.info("üöÄ Starting AI Worker (LangGraph 1.0.0)")
        logger.info(f"   Shards: {config.SHARD_COUNT}")
        logger.info(f"   LLM: {config.LLM_MODEL}")
        logger.info(f"   Concurrency: {config.LLM_CONCURRENCY}")
        logger.info("=" * 80)
        
        #asyncio.run(start_consumer())
        loop = asyncio.SelectorEventLoop(selectors.SelectSelector()) # √©p window d√πng SelectorEventLoop
        asyncio.set_event_loop(loop) # kh·ªüi t·∫°o event loop
        loop.run_until_complete(start_consumer()) # ch·∫°y ch√≠nh, ch·ªù ƒë·∫øn khi h√†m start_consumer k·∫øt th√∫c 
        # h√†m start_consumer s·∫Ω ch·∫°y v√† g√°n task v√†o queue ch·ªù t√≠n hi·ªáu d·ª´ng 
        #Windows m·∫∑c ƒë·ªãnh d√πng ProactorEventLoop, nh∆∞ng psycopg async kh√¥ng t∆∞∆°ng th√≠ch. √©p bu·ªôc d√πng SelectorEventLoop ( do kh√¥ng t∆∞∆°ng th√≠ch v·ªõi psycopg th∆∞ vi·ªán PostGreSQL async)
    except KeyboardInterrupt:
        logger.info("üßπ Worker stopped by user")
    except Exception as e:
        logger.error(f"üî• Worker crashed: {e}", exc_info=True)
        sys.exit(1)
    #th√™m c∆° ch·∫ø x√≥a checkpointer khi ƒë·ªß 1 ng√†y kh√¥ng s·ª≠ d·ª•ng ƒë·ªÉ tr√°nh t·ªën dung l∆∞·ª£ng db


    # th·ª±c ra m·ªói l·∫ßn ch·∫°y (python main.py) ƒë√≥ m·ªõi l√† 1 worker, h·ª£p l√Ω khi s·ª≠ d·ª•ng lock
    # 1 worker s·∫Ω t·∫°o ra 8 task, m·ªói task l·∫Øng nghe 1 queue c·ªë ƒë·ªãnh, v√† n√≥ s·∫Ω x·ª≠ l√Ω request t·ª´ queue ƒë√≥
    # m·ªói task v·ªõi c·∫•u h√¨nh hi·ªán t·∫°i ƒëang ƒë∆∞·ª£c ph√©p l·∫•y 10 message c√πng l√∫c, v√† x·ª≠ l√Ω ƒë·ªìng th·ªùi 5 request LLM c√πng l√∫c ( n·∫øu c√°c task kh√°c kh√¥ng s·ª≠ d·ª•ng llm)
    # v√† 5 llm ƒë∆∞·ª£c khai b√°o ƒë√≥, c√°c task s·∫Ω s·ª≠ d·ª•ng chung v·ªõi nhau 
    
    #Yield & Resume: ƒë√¢y l√† c∆° ch·∫ø gi√∫p cho h√†m b·∫•t ƒë·ªìng b·ªô ho·∫°t ƒë·ªông tr√™n m·ªôt thread ( lu·ªìng)