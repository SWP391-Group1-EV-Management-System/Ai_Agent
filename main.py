import asyncio, json, uuid, os, logging, sys, signal
import aio_pika
import redis.asyncio as aioredis
from fastapi import FastAPI, Request
from dotenv import load_dotenv
from langchain.agents import create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMMathChain
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.tools import Tool
from tools.API_BE import listUser_api, add_user_to_api
from data.personality_config import SYSTEM_PROMPT, PERSONALITY_CONFIG
from data.training_examples import GREETING_EXAMPLES, SUPPORT_EXAMPLES, MATH_EXAMPLES
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, create_react_agent
#from langgraph.checkpoint.memory import MemorySaver
from functools import partial
from hashlib import md5
from typing import Dict, Any, List
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, text
from concurrent.futures import ThreadPoolExecutor
from contextlib import suppress
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

load_dotenv()
RABBIT_URL = os.getenv("RABBITMQ_URL", "amqp://guest:guest@localhost/")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost")
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:12345@localhost:5432/DataAIAgent")
SHARD_COUNT = int(os.getenv("SHARD_COUNT", "8"))   # s·ªë queue shard
AI_QUEUE_PREFIX = "ai_jobs.shard_"
DLX = "ai_jobs.dlx"  # dead-letter exchange

# async DB
engine = create_async_engine(DATABASE_URL, pool_size=20, max_overflow=10)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
#executor = ThreadPoolExecutor(max_workers=5)
app = FastAPI(title="AI Message Gateway")
# LLM stub (replace with real client). If sync -> run_in_executor
class DummyLLM:
    def predict(self, prompt: str):
        return f"LLM reply to: {prompt[:120]}"
USE_DUMMY = os.getenv("USE_DUMMY", "false").lower() == "true"

if USE_DUMMY:
    llm = DummyLLM()
else:
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.3,
        google_api_key=os.getenv("GOOGLE_API_KEY"),
        max_tokens=2048,
        top_p=0.95,
    )
llm_sem = asyncio.Semaphore(int(os.getenv("LLM_CONCURRENCY", "4")))
# --- Thi·∫øt l·∫≠p logger to√†n c·ª•c ---
# ‚úÖ T·∫°o LCEL Chain v·ªõi history support
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† h·ªØu √≠ch."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{user_input}")
])
# ‚úÖ Chain: prompt | llm | parser
chat_chain = chat_prompt | llm | StrOutputParser()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [%(name)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler("ai_worker.log", encoding="utf-8"),  # l∆∞u file log
        logging.StreamHandler(sys.stdout)                        # in ra console
    ]
)

logger = logging.getLogger(__name__)
async def call_llm_with_history(user_input: str, history: List[Dict[str, str]]) -> str:
    """
    G·ªçi LLM v·ªõi chat history s·ª≠ d·ª•ng LCEL Chain
    
    Args:
        user_input: Tin nh·∫Øn m·ªõi t·ª´ user
        history: List c√°c messages tr∆∞·ªõc ƒë√≥ [{"role": "user/assistant", "content": "..."}]
    
    Returns:
        C√¢u tr·∫£ l·ªùi t·ª´ LLM
    """
    try:
        if USE_DUMMY:
            # ‚úÖ Th√™m delay ƒë·ªÉ simulate LLM call th·∫≠t
            await asyncio.sleep(2)
            return f"ƒê√¢y l√† c√¢u tr·∫£ l·ªùi gi·∫£ cho: {user_input[:50]}..."
        
        # ‚úÖ Convert history t·ª´ DB format sang LangChain messages
        chat_history = []
        for msg in history[-10:]:  # Ch·ªâ l·∫•y 10 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám tokens
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                chat_history.append(AIMessage(content=msg["content"]))
        
        # ‚úÖ Invoke chain v·ªõi ainvoke() - async native
        response = await chat_chain.ainvoke({
            "chat_history": chat_history,
            "user_input": user_input
        })
        
        return response
    
    except Exception as e:
        logger.error(f"‚ùå LLM call failed: {e}", exc_info=True)
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y l√∫c n√†y."
# util: shard key -> queue name
def user_shard_queue(user_id: str) -> str:
    h = int(md5(user_id.encode()).hexdigest()[:8], 16)
    return f"{AI_QUEUE_PREFIX}{h % SHARD_COUNT}"

# initialize connections
async def init():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi RabbitMQ, Redis v√† PostgreSQL"""
    try:
        logger.info("üîå ƒêang k·∫øt n·ªëi RabbitMQ & Redis...")

        # K·∫øt n·ªëi RabbitMQ
        conn = await aio_pika.connect_robust(RABBIT_URL)
        channel = await conn.channel()
        await channel.set_qos(prefetch_count=10)  # Gi·ªõi h·∫°n m·ªói consumer t·ªëi ƒëa 10 task song song

        # K·∫øt n·ªëi Redis
        redis = await aioredis.from_url(REDIS_URL, decode_responses=True)

        logger.info("‚úÖ K·∫øt n·ªëi RabbitMQ, Redis, PostgreSQL th√†nh c√¥ng.")
        return conn, channel, redis, engine

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o k·∫øt n·ªëi: {e}", exc_info=True)
        raise

# idempotency check
async def is_done(redis, job_id):
    return await redis.sismember("jobs:completed", job_id)

async def mark_done(redis, job_id, ttl=86400):
    await redis.sadd("jobs:completed", job_id)
    await redis.expire("jobs:completed", ttl)

# Redlock-ish simple lock (use redis-py-redlock for production or Redis docs)
async def acquire_lock(redis, key, ttl=30_000):
    ok = await redis.set(key, "1", nx=True, px=ttl)
    return ok

async def release_lock(redis, key):
    await redis.delete(key)

# checkpointer operations
async def get_cp(redis, user_id):
    raw = await redis.get(f"checkpointer:{user_id}")
    return json.loads(raw) if raw else {"in_progress":[], "completed":[]}

async def save_cp(redis, user_id, cp):
    await redis.set(f"checkpointer:{user_id}", json.dumps(cp), ex=3600)

# core job handler
async def handle_message(redis, payload: Dict[str,Any], message: aio_pika.IncomingMessage):
    job_id = payload["job_id"]
    user_id = payload["user_id"]
    user_message = payload["text"]  # ‚úÖ ƒê·ªïi t√™n t·ª´ text ‚Üí user_message
    
    # Idempotency: skip if already processed
    if await is_done(redis, job_id):
        logger.info(f"Skip completed job {job_id}")
        await message.ack()
        return
    
    # Acquire distributed lock
    lock_key = f"lock:job:{job_id}"
    if not await acquire_lock(redis, lock_key, ttl=30000):
        logger.info(f"Job {job_id} ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi worker kh√°c")
        await message.ack()
        return

    try:
        # Double-check idempotency sau khi c√≥ lock
        if await is_done(redis, job_id):
            logger.info(f"Skip completed job {job_id} (double-check)")
            await message.ack()
            return
        
        # Mark in-progress in checkpointer
        cp = await get_cp(redis, user_id)
        if job_id in cp.get("completed", []):
            await message.ack()
            return
        if job_id not in cp.get("in_progress", []):
            cp.setdefault("in_progress", []).append(job_id)
            await save_cp(redis, user_id, cp)

        # ‚úÖ Load history from Postgres
        async with AsyncSessionLocal() as db:
            res = await db.execute(
                text("SELECT role, content FROM chat_messages WHERE user_id=:u ORDER BY created_at ASC LIMIT 100"),  # ‚úÖ text() gi·ªù ho·∫°t ƒë·ªông ƒë√∫ng
                {"u": user_id}
            )
            rows = res.fetchall()
            history = [{"role": r[0], "content": r[1]} for r in rows]

        # ‚úÖ G·ªçi LLM v·ªõi history s·ª≠ d·ª•ng LCEL Chain (limit concurrency)
        async with llm_sem:
            reply = await call_llm_with_history(user_message, history)  # ‚úÖ D√πng user_message
            logger.info(f"ü§ñ Gemini reply for job {job_id}: {reply[:100]}...")

        # ‚úÖ Save to DB (user message + assistant reply)
        async with AsyncSessionLocal() as db:
            await db.execute(
                text("INSERT INTO chat_messages (user_id, role, content) VALUES (:u, :r, :c)"),
                {"u": user_id, "r": "user", "c": user_message}  # ‚úÖ D√πng user_message
            )
            await db.execute(
                text("INSERT INTO chat_messages (user_id, role, content) VALUES (:u, :r, :c)"),
                {"u": user_id, "r": "assistant", "c": reply}
            )
            await db.commit()

        # Mark completed & idempotent
        cp = await get_cp(redis, user_id)
        if job_id in cp.get("in_progress", []):
            cp["in_progress"].remove(job_id)
        cp.setdefault("completed", []).append(job_id)
        await save_cp(redis, user_id, cp)
        await mark_done(redis, job_id)

        # If no in_progress left, clear cp key
        if not cp.get("in_progress"):
            await redis.delete(f"checkpointer:{user_id}")

        await message.ack()
        logger.info(f"‚úÖ Job {job_id} ho√†n th√†nh th√†nh c√¥ng")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω job {job_id}: {e}", exc_info=True)
        
        # Retry logic with exponential backoff
        headers = dict(message.headers) if message.headers else {}
        retries = int(headers.get("x-retries", 0))
        
        if retries < 5:
            headers["x-retries"] = retries + 1
            await message.nack(requeue=True)
            logger.warning(f"‚ö†Ô∏è Job {job_id} retry {retries + 1}/5")
        else:
            # Publish to DLX after 5 retries
            dlx_msg = aio_pika.Message(body=message.body, headers=headers)
            await message.channel.default_exchange.publish(dlx_msg, routing_key="ai_jobs.dlq")
            await message.ack()
            logger.error(f"‚ùå Job {job_id} failed after 5 retries, sent to DLQ")
            
    finally:
        await release_lock(redis, lock_key)

# consumer starter: bind to the appropriate shard queue and consume with prefetch set
async def consume_shard(redis, q, shard_id, stop_event: asyncio.Event):
    logger.info(f"üöÄ Worker shard-{shard_id} b·∫Øt ƒë·∫ßu ti√™u th·ª• h√†ng ƒë·ª£i '{q.name}'")
    #  ƒê·ªãnh nghƒ©a callback x·ª≠ l√Ω message
    async def on_message(msg: aio_pika.IncomingMessage):
        """Callback ƒë∆∞·ª£c g·ªçi khi c√≥ message m·ªõi t·ª´ queue"""
        async with msg.process(ignore_processed=True):
            try:
                payload = json.loads(msg.body.decode())
                logger.info(f"[shard-{shard_id}] üì© Nh·∫≠n job: {payload}")
                
                # X·ª≠ l√Ω message (handle_message s·∫Ω t·ª± ACK/NACK)
                await handle_message(redis, payload, msg)
                
            except json.JSONDecodeError:
                logger.error(f"[shard-{shard_id}] ‚ùå L·ªói JSON: {msg.body}")
                raise
                
            except Exception as e:
                logger.error(f"[shard-{shard_id}] ‚ùå L·ªói x·ª≠ l√Ω: {e}", exc_info=True)
                raise
    
    #  ƒêƒÉng k√Ω consumer v·ªõi callback
    consumer_tag = await q.consume(on_message, no_ack=False)
    logger.info(f"‚úÖ Consumer shard-{shard_id} ƒë√£ ƒëƒÉng k√Ω (tag: {consumer_tag})")
    
    try:
        #  Ch·ªù cho ƒë·∫øn khi stop_event ƒë∆∞·ª£c set
        await stop_event.wait()
        logger.info(f"[shard-{shard_id}] üõë Nh·∫≠n t√≠n hi·ªáu d·ª´ng")
        
    except asyncio.CancelledError:
        logger.info(f"[shard-{shard_id}] üõë Task b·ªã cancel")
        raise
        
    finally:
        #  H·ªßy consumer khi d·ª´ng
        try:
            await q.cancel(consumer_tag)
            logger.info(f"[shard-{shard_id}] ‚úÖ ƒê√£ h·ªßy consumer")
        except Exception as e:
            logger.warning(f"[shard-{shard_id}] ‚ö†Ô∏è L·ªói khi h·ªßy consumer: {e}")
        
        logger.info(f"üõë Worker shard-{shard_id} d·ª´ng ti√™u th·ª• '{q.name}'")
async def start_consumer():
    conn, channel, redis, engine = await init()

    stop_event = asyncio.Event()
    tasks = []
    # T·∫°o consumer cho t·ª´ng shard
    for shard in range(SHARD_COUNT):
        qname = f"{AI_QUEUE_PREFIX}{shard}"
        queue = await channel.declare_queue(qname, durable=True)
        tasks.append(asyncio.create_task(consume_shard(redis, queue, shard, stop_event)))

    def signal_handler(sig, frame):
        logger.info("üõë Nh·∫≠n t√≠n hi·ªáu d·ª´ng, ƒëang shutdown...")
        stop_event.set()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # --- ch·∫°y cho t·ªõi khi b·ªã d·ª´ng ---
    '''
    await stop_event.wait()
    logger.info("‚èπ ƒêang h·ªßy c√°c consumer tasks...")

    for t in tasks:
        t.cancel()

    with suppress(asyncio.CancelledError):
        await asyncio.gather(*tasks, return_exceptions=True)
    '''
    try:
        await asyncio.gather(*tasks, return_exceptions=True)
    except asyncio.CancelledError:
        logger.info("‚èπ ƒêang h·ªßy c√°c consumer tasks...")
    # --- cleanup ---
    logger.info("üíæ ƒêang ƒë√≥ng k·∫øt n·ªëi...")
    await channel.close()
    await conn.close()
    #await redis.close()
    await redis.aclose()
    logger.info("‚úÖ Worker shutdown ho√†n t·∫•t.")

# FastAPI app cho UI g·ª≠i message
@app.post("/send_message")
async def send_message(request: Request):
    """Nh·∫≠n message t·ª´ UI v√† g·ª≠i v√†o RabbitMQ shard queue"""
    data = await request.json()
    user_id = data.get("user_id")
    text = data.get("message")

    if not user_id or not text:
        return {"status": "error", "detail": "Thi·∫øu user_id ho·∫∑c message"}

    try:
        # Kh·ªüi t·∫°o k·∫øt n·ªëi nhanh t·ªõi RabbitMQ + Redis
        conn, channel, redis, _ = await init()

        # Sinh job_id duy nh·∫•t
        job_id = str(uuid.uuid4())

        # T·∫°o payload g·ª≠i cho worker
        payload = {"job_id": job_id, "user_id": user_id, "text": text}
        body = json.dumps(payload).encode()

        # X√°c ƒë·ªãnh shard queue
        queue_name = user_shard_queue(user_id)

        # G·ª≠i message v√†o queue
        await channel.default_exchange.publish(
            aio_pika.Message(body=body, delivery_mode=aio_pika.DeliveryMode.PERSISTENT),
            routing_key=queue_name
        )

        logger.info(f"üì® G·ª≠i job {job_id} c·ªßa user {user_id} v√†o queue '{queue_name}'")

        # ƒê√≥ng k·∫øt n·ªëi sau khi publish
        await channel.close()
        await conn.close()
        await redis.close()

        return {"status": "ok", "job_id": job_id, "queue": queue_name}

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi g·ª≠i message: {e}", exc_info=True)
        return {"status": "error", "detail": str(e)}
   
    

if __name__ == "__main__":
    try:
        asyncio.run(start_consumer())
    except KeyboardInterrupt:
        logger.info("üßπ D·ª´ng worker theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")
    except Exception as e:
        logger.error(f"üî• Worker crashed: {e}", exc_info=True)
