# ==================== INFRASTRUCTURE ====================
# Backend API URL
BACKEND_URL=http://localhost:8080

# RabbitMQ Connection (Docker internal network)
RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/

# Redis Connection (Docker internal network)
REDIS_URL=redis://redis-ai:6379

# PostgreSQL Connection (Docker internal network - async driver)
DATABASE_URL=postgresql+asyncpg://postgres:123456@postgres-ai:5432/DataAIAgent

# ==================== SHARDING & QUEUES ====================
# Number of queue shards for horizontal scaling (Production: tăng để handle nhiều request)
SHARD_COUNT=8

# Queue prefix for AI jobs
AI_QUEUE_PREFIX=ai_jobs_

# Dead Letter Exchange name
DLX=ai_dlx

# ==================== PERFORMANCE TUNING ====================
# Max concurrent LLM calls (Production: tăng để tận dụng server)
LLM_CONCURRENCY=10

# RabbitMQ prefetch count per consumer (Production: tăng để xử lý nhanh hơn)
PREFETCH_COUNT=20

# PostgreSQL connection pool (Production: tăng để handle nhiều connections)
DB_POOL_SIZE=30
DB_MAX_OVERFLOW=20

# ==================== RETRY & TIMEOUT ====================
# Maximum retry attempts before sending to DLQ
MAX_RETRIES=5

# Distributed lock TTL in milliseconds
LOCK_TTL_MS=30000

# Job execution timeout in seconds
JOB_TIMEOUT_SEC=300

# ==================== LLM CONFIGURATION ====================
# Use dummy LLM for testing (no API calls)
USE_DUMMY=false

# Google Gemini API Key
GOOGLE_API_KEY=AIzaSyB7Eli5Z4S0FkEIO2VSSdKltfpvHtdUGfA

# Model selection
# Options: gemini-2.0-flash-exp, gemini-2.5-flash, gemini-1.5-pro
LLM_MODEL=gemini-2.0-flash-exp

# Temperature (0.0 - 1.0)
LLM_TEMPERATURE=0.3

# Max tokens per response
LLM_MAX_TOKENS=2048

# ==================== MEMORY & HISTORY ====================
# Max conversation history messages to load (Production: tăng để lưu nhiều context)
MAX_HISTORY_MESSAGES=50

# Checkpoint TTL in seconds (Production: tăng để giữ context lâu hơn)
CHECKPOINT_TTL_SEC=7200


# ==================== LANGCHAIN & MONITORING ====================
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=""
LANGCHAIN_PROJECT=""
# Dùng cho checkpointer (PSYCOPG DSN - Production)
CHECKPOINTER_DB_DSN=postgresql://postgres:123456@postgres-ai:5432/DataAIAgent

# Sentry (for error tracking) - Optional
SENTRY_DSN=

# Prometheus metrics port - Optional
METRICS_PORT=9090

# API check token URL (Docker internal network)
SPRING_CHECK_TOKEN_URL=http://backend-api:8080/api/auth/check-token

# =================================================================
# ADDITIONAL PRODUCTION SETTINGS
# =================================================================
AI_POSTGRES_PASSWORD=123456
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest